{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "-0.5 0.5\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# lets load up the handwritten digit dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "digit = 4\n",
    "x_digits = X[y == digit]\n",
    "for i in range(25):\n",
    "    img = x_digits[i].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(1437,)\n",
      "(360, 64)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env\n",
    "\n",
    "__filename__ = \"latex_doc_strings.py\"\n",
    "__author__ = \"L.J. Brown\"\n",
    "\n",
    "import re\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "def extract_doc_latex(method):\n",
    "    \"\"\"\n",
    "        looks for latex in doc strings of passes method.\n",
    "        \n",
    "        Format example for docstring:\n",
    "        \n",
    "            r\\\"\"\" \n",
    "                    Foo Method Description \n",
    "\n",
    "                :latex: MULTI-LINE\n",
    "                        LATEX EXPRESSION\n",
    "                        INSERTED HERE\n",
    "                        \n",
    "                :param bar: (example parameter)\n",
    "                :returns: (whatever) \n",
    "\n",
    "                * [Notes] : \n",
    "                \n",
    "                    - latex code must be followed by \n",
    "                        at least one ':'(like the one \n",
    "                        leading param above)\n",
    "                        \n",
    "                    - doc string must be proceeded by r\\\"\"\" \n",
    "            \\\"\"\"\n",
    "            \n",
    "        :param: method with string literal doc string containing latex\n",
    "        :returns: latex string literal\n",
    "        \n",
    "    \"\"\"\n",
    "    p = re.compile('(?:\\:latex\\:)([\\s\\S]+?)(?:\\:)', re.MULTILINE)\n",
    "    m = p.search(method.__doc__)\n",
    "    return m.group(1)\n",
    "\n",
    "def latex_doc(method):\n",
    "    \"\"\"\n",
    "    returns rendered latex from doc string of method passed as parameter\n",
    "\n",
    "    Format example for docstring:\n",
    "\n",
    "            r\\\"\"\" \n",
    "                    Foo Method Description \n",
    "\n",
    "                :latex: MULTI-LINE\n",
    "                        LATEX EXPRESSION\n",
    "                        INSERTED HERE\n",
    "\n",
    "                :param bar: (example parameter)\n",
    "                :returns: (whatever) \n",
    "\n",
    "                * [Notes] : \n",
    "\n",
    "                    - latex code must be followed by \n",
    "                        at least one ':'(like the one \n",
    "                        leading param above)\n",
    "\n",
    "                    - doc string must be proceeded by r\\\"\"\" \n",
    "            \\\"\"\"\n",
    "\n",
    "    :param: method with string literal doc string containing latex\n",
    "    :returns: formated latex expression\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    latex_string = extract_doc_latex(method)\n",
    "    return display(Math(latex_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \n",
       "       \n",
       "               \\text{Takes as input paramater a matrix $\\mathbf{A}_{s \\times n}$,} \\\\\n",
       "               \\text{ }\\\\\n",
       "               \\mathbf{A}_{ input } =\n",
       "                \\begin{bmatrix}\n",
       "                     \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
       "                                & \\vdots     &            \\\\\n",
       "                     \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
       "                \\end{bmatrix}\n",
       "                \\text{ }\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\text{Where '$s$' is the number of instances being fed to the network.}\\\\\n",
       "                \\\\\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\text{Adds a bias column to the first column of $\\mathbf{A}$ and returns the new matrix $\\mathbf{A'}_{s \\times n+1}$, }\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\mathbf{A'}_{column \\, bias} =\n",
       "                \\begin{bmatrix}\n",
       "                     1     &  \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
       "                   \\       &             & \\vdots     &            \\\\\n",
       "                     1     &  \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
       "                \\end{bmatrix}\n",
       "                \n",
       "       $$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_bias_unit(A):\n",
    "    r\"\"\"\n",
    "        Add  bias units (column of 1s) to array at index 0\n",
    "        \n",
    "       :latex: \n",
    "       \n",
    "               \\text{Takes as input paramater a matrix $\\mathbf{A}_{s \\times n}$,} \\\\\n",
    "               \\text{ }\\\\\n",
    "               \\mathbf{A}_{ input } =\n",
    "                \\begin{bmatrix}\n",
    "                     \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
    "                                & \\vdots     &            \\\\\n",
    "                     \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
    "                \\end{bmatrix}\n",
    "                \\text{ }\\\\\n",
    "                \\text{ }\\\\\n",
    "                \\text{Where '$s$' is the number of instances being fed to the network.}\\\\\n",
    "                \\\\\\\\\n",
    "                \\text{ }\\\\\n",
    "                \\text{Adds a bias column to the first column of $\\mathbf{A}$ and returns the new matrix $\\mathbf{A'}_{s \\times n+1}$, }\\\\\n",
    "                \\text{ }\\\\\n",
    "                \\mathbf{A'}_{column \\, bias} =\n",
    "                \\begin{bmatrix}\n",
    "                     1     &  \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
    "                   \\       &             & \\vdots     &            \\\\\n",
    "                     1     &  \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
    "                \\end{bmatrix}\n",
    "                \n",
    "       :param A: numpy array to add bias row or column to\n",
    "    \"\"\"\n",
    "    ones = np.ones((A.shape[0], 1))\n",
    "    A_new = np.hstack((ones, A))\n",
    "\n",
    "    return A_new\n",
    "\n",
    "latex_doc(add_bias_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\text{ Constructs Weight Matrix $\\mathbf{W}_{n \\times m}^{(l)}$. } \\\\ \\\\\n",
       "               \\text{ }\\\\\n",
       "               \\text{ Initalizes weights using a random uniform distribution ranging from $[-1,1]$.} \\\\\n",
       "               \\text{ Number of input units, } n  \\\\\n",
       "               \\text{ Number of output units, } m \\\\\n",
       "               \\text{ }\\\\\n",
       "               \\mathbf{W}^{(l)} =\n",
       "               \\begin{bmatrix}\n",
       "                    b_{0}   & \\ldots  & b_{m-1} \\\\\n",
       "                    w_{1,0} & \\ldots  & w_{1,m-1} \\\\\n",
       "                    \\vdots  & \\ldots  & \\vdots \\\\\n",
       "                    w_{n-1,0} & \\ldots  & w_{n-1,m-1}\n",
       "               \\end{bmatrix} \\\\\n",
       "    \\text{ This matrix is used in conjustion with layer $l's$ activation matrix $\\mathbf{A}^{(l)}$ }\\\\\n",
       "    \\text{ to calculate the input matrix $\\mathbf{Z}^{(l)}$ to the next activation function.} \\\\\n",
       "    \\\\\n",
       "    \\text{ }\\\\\n",
       "    \\text{ }\\\\\n",
       "    \\text{If '$s$' be the number of instances being fed to the network then,}\\\\\n",
       "    \\mathbf{Z}_{s \\times m}^{(l)} = \\mathbf{A}_{s \\times n}^{(l)} \\mathbf{W}_{n \\times m}^{(l)} \\\\\n",
       "    \\text{ }\\\\\n",
       "    \\mathbf{Z} =\n",
       "    \\begin{bmatrix} \n",
       "            \\text{---} &  z^{(0)}   & \\text{---} \\\\\n",
       "                       & \\ldots     &            \\\\\n",
       "            \\text{---} &  z^{(s-1)} & \\text{---} \\\\\n",
       "    \\end{bmatrix} , \\;\n",
       "    \n",
       "    \\begin{bmatrix} \n",
       "            1       & \\text{---} &  a^{(0)}   & \\text{---} \\\\\n",
       "           \\vdots   &            & \\ldots     &            \\\\\n",
       "            1       & \\text{---} &  a^{(s-1)} & \\text{---} \\\\\n",
       "    \\end{bmatrix}\n",
       "    = \\mathbf{A} \\\\\n",
       "    \n",
       "    \\text{ }\\\\\n",
       "    \\text{Returns randomly initalized weight matrix $\\mathbf{W}_{n \\times m}^{(l)}.$}\n",
       "    \n",
       "    \n",
       "      $$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_weight_matrix(num_input_units, num_output_units):\n",
    "    r\"\"\"\n",
    "        Build Weight Matrix W for layer l given number of inputs and outputs.\n",
    "        Initialize weights with small random numbers.\n",
    "          \n",
    "       :latex: \\text{ Constructs Weight Matrix $\\mathbf{W}_{n \\times m}^{(l)}$. } \\\\ \\\\\n",
    "               \\text{ }\\\\\n",
    "               \\text{ Initalizes weights using a random uniform distribution ranging from $[-1,1]$.} \\\\\n",
    "               \\text{ Number of input units, } n  \\\\\n",
    "               \\text{ Number of output units, } m \\\\\n",
    "               \\text{ }\\\\\n",
    "               \\mathbf{W}^{(l)} =\n",
    "               \\begin{bmatrix}\n",
    "                    b_{0}   & \\ldots  & b_{m-1} \\\\\n",
    "                    w_{1,0} & \\ldots  & w_{1,m-1} \\\\\n",
    "                    \\vdots  & \\ldots  & \\vdots \\\\\n",
    "                    w_{n-1,0} & \\ldots  & w_{n-1,m-1}\n",
    "               \\end{bmatrix} \\\\\n",
    "    \\text{ This matrix is used in conjustion with layer $l's$ activation matrix $\\mathbf{A}^{(l)}$ }\\\\\n",
    "    \\text{ to calculate the input matrix $\\mathbf{Z}^{(l)}$ to the next activation function.} \\\\\n",
    "    \\\\\n",
    "    \\text{ }\\\\\n",
    "    \\text{ }\\\\\n",
    "    \\text{If '$s$' be the number of instances being fed to the network then,}\\\\\n",
    "    \\mathbf{Z}_{s \\times m}^{(l)} = \\mathbf{A}_{s \\times n}^{(l)} \\mathbf{W}_{n \\times m}^{(l)} \\\\\n",
    "    \\text{ }\\\\\n",
    "    \\mathbf{Z} =\n",
    "    \\begin{bmatrix} \n",
    "            \\text{---} &  z^{(0)}   & \\text{---} \\\\\n",
    "                       & \\ldots     &            \\\\\n",
    "            \\text{---} &  z^{(s-1)} & \\text{---} \\\\\n",
    "    \\end{bmatrix} , \\;\n",
    "    \n",
    "    \\begin{bmatrix} \n",
    "            1       & \\text{---} &  a^{(0)}   & \\text{---} \\\\\n",
    "           \\vdots   &            & \\ldots     &            \\\\\n",
    "            1       & \\text{---} &  a^{(s-1)} & \\text{---} \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\mathbf{A} \\\\\n",
    "    \n",
    "    \\text{ }\\\\\n",
    "    \\text{Returns randomly initalized weight matrix $\\mathbf{W}_{n \\times m}^{(l)}.$}\n",
    "    \n",
    "    \n",
    "      :param num_input_units: integer, number of rows in W representing the number of outputs from the previous layer (neglecting the bias terms), or the number of features for the network inputs (again neglecting the bias terms).\n",
    "      :param num_output_units: integer, number of columns in W representing the number input units for the next layer (neglecting the bias terms), or the number of classes for the networks output.\n",
    "    \"\"\"\n",
    "    W_num_elems = num_input_units * num_output_units\n",
    "    \n",
    "    W = np.random.uniform(-1.0, 1.0, size=W_num_elems)\n",
    "    W = W.reshape(num_input_units, num_output_units) # reshape to be W\n",
    "    \n",
    "    return W\n",
    "\n",
    "latex_doc(build_weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward And DNN Network Dimensions\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Size variable definitions\n",
    "\n",
    "Let $L$ be the total number of layers in the network, \n",
    "$$ l = 0, 1, ..., L_{-1} $$\n",
    "\n",
    "We'll denote the layer for particular component of the network with the superscript $ \\mathbf{\\xi}^{(l)} $ if it is a function or a matrix. If the component is a particular instance traveling through the network then the superscript refers to the instance number.\n",
    "\n",
    "$$\n",
    " s \\, \\sim \\, \\text{ The number of input samples to the network : n_samples } \\\\\n",
    " f \\, \\sim \\, \\text{ The number of features for every sample : n_features } \\\\\n",
    " c \\, \\sim \\, \\text{ The number of classes the network can classify : n_classes } \\\\\n",
    " L \\, \\sim \\, \\text{ The total number of layers in the network : n_layers } \\\\\n",
    " l \\, \\sim \\, \\text{ The particular layer of the component in focus } \\\\\n",
    " in^{(l)} \\, \\sim \\, \\text{ The number of input units to layer $l$ : layers[ l ].n_inputs } \\\\\n",
    " out^{(l)} \\, \\sim \\, \\text{ the number of output units from layer $l$ : lares[ l ].n_outputs } \\\\\n",
    "$$\n",
    "\n",
    "### Network Components And Dimensions\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_{( \\, s \\, \\times \\, f \\, ) } \\, \\sim \\, \\text{ Input samples to the network } \\\\\n",
    "\\mathbf{\\tilde{Y}}_{( \\, s \\, \\times \\, c \\, ) } \\, \\sim \\, \\text{ The out put of the network } \\\\\n",
    "\\mathbf{A}^{(l)}_{ ( \\, s \\, \\times \\, in^{(l)} \\, )} \\, \\sim \\, \\text{ The activations traveling through the network at layer $l$ } \\\\\n",
    "\\mathbf{A'}^{(l)}_{( \\, s \\, \\times \\, in^{(l)}+1 \\, ) } \\, \\sim \\, \\text{ The activations traveling through the network at layer $l$ plus the bias column of ones added to $ \\mathbf{A}^{(l)} $ at index $0$ } \\\\\n",
    "\\mathbf{W}^{(l)}_{ ( \\, in^{(l)}+1 \\, \\times \\, out^{(l)} \\, ) } \\, \\sim \\, \\text{ The weight matrix of the network at layer $l$ } \\\\\n",
    "\\mathbf{Z}^{(l)}_{( \\, s \\, \\times \\, out^{(l)} \\, ) } \\, \\sim \\, \\text{ Intermediate Component, \n",
    "$\\mathbf{Z}^{(l)} = \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)}  $ } \\\\\n",
    "\\phi^{(l)}  \\, \\sim \\, \\text{ The activation function of layer $l$. Layer $l$'s output is defined as $\\phi^{(l)}( \\, \\mathbf{Z}^{(l)} \\, )$ } \\\\\n",
    "$$\n",
    "\n",
    "### Detailed Description of Network Components\n",
    "$$\n",
    "\\text{ $\\mathbf{X}$ is the input to the network.} \\\\ \n",
    "\\text{ It is a matrix of row vectors. Each row is a particular instance denoted by it's superscript. } \\\\\n",
    "\\mathbf{X}_{( \\, s \\, \\times \\, f \\, ) } =\n",
    "    \\begin{bmatrix}\n",
    "         \\text{---} & x^{(0)}    & \\text{---} \\\\\n",
    "                    & \\vdots     &            \\\\\n",
    "         \\text{---} & x^{(s-1)}  & \\text{---} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ $\\mathbf{A}^{(l)}$ is the input to layer $l$ of the network. } \\\\\n",
    "\\text{ $\\mathbf{A}^{(l)}$ is again a matrix of row vectors corresponsing to a particular samples activations traveling through the network. } \\\\\n",
    "\\mathbf{A}^{(l)}_{( \\, s \\, \\times \\, in^{(l)} \\, ) } =\n",
    "    \\begin{bmatrix}\n",
    "         \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
    "                    & \\vdots     &            \\\\\n",
    "         \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ $\\mathbf{A'}^{(l)}$ is the input to layer $l$ of the network with the addition of a bias column of $1$'s at index $0$. } \\\\\n",
    "\\mathbf{A'}^{(l)}_{( \\, s \\, \\times \\, in^{(l)}+1 \\, ) } =\n",
    "    \\begin{bmatrix} \n",
    "            1       & \\text{---} &  a^{(0)}   & \\text{---} \\\\\n",
    "           \\vdots   &            & \\ldots     &            \\\\\n",
    "            1       & \\text{---} &  a^{(s-1)} & \\text{---} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ $\\mathbf{W}^{(l)}$ is the weight matrix of layer $l$ of the network. } \\\\\n",
    "\\text{ The first row of $\\mathbf{W}^{(l)}$ is made up of biases not connected to the input for the layer. } \\\\\n",
    "\\mathbf{W}^{(l)}_{( \\, in^{(l)}+1 \\, \\times \\, out^{(l)} \\, ) } =\n",
    "   \\begin{bmatrix}\n",
    "        b_{0}   & \\ldots  & b_{out^{(l)}-1} \\\\\n",
    "        w_{1,0} & \\ldots  & w_{1, \\, out^{(l)} -1} \\\\\n",
    "        \\vdots  & \\ldots  & \\vdots \\\\\n",
    "        w_{in^{(l)},0} & \\ldots  & w_{in^{(l)}, \\, out^{(l)} -1 }\n",
    "   \\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ $\\mathbf{Z}^{(l)}$ is an intermidiate matrix for layer $l$.} \\\\ \n",
    "\\text{ It is a matrix of row vectors. Each row corresponds to a particular instance of a sample traveling through the network. } \\\\\n",
    "\\mathbf{Z}^{(l)}_{( \\, s \\, \\times \\, out^{(l)} \\, ) } =\n",
    "    \\begin{bmatrix}\n",
    "         \\text{---} & z^{(0)}    & \\text{---} \\\\\n",
    "                    & \\vdots     &            \\\\\n",
    "         \\text{---} & z^{(s-1)}  & \\text{---} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ $\\mathbf{\\tilde{Y}}$ is the output of the network.} \\\\ \n",
    "\\text{ It is a matrix of row vectors. Each row corresponds to the predictions for a sample traveling passed to the matrix.} \\\\\n",
    "\\text{ Each column represents a class. Each element a score for that class. } \\\\\n",
    "\\mathbf{\\tilde{Y}}_{( \\, s \\, \\times \\, c \\, ) } =\n",
    " \\begin{bmatrix}\n",
    "         \\text{---} & \\tilde{y}^{(0)}    & \\text{---} \\\\\n",
    "                    & \\vdots     &            \\\\\n",
    "         \\text{---} & \\tilde{y}^{(s-1)}  & \\text{---} \\\\\n",
    "\\end{bmatrix}, \\;\n",
    "\\begin{bmatrix}\n",
    "    \\vert             &        & \\vert       \\\\\n",
    "    score_{class_0}   & \\ldots & score_{class_{c-1}} \\\\\n",
    "    \\vert             &        & \\vert\n",
    "\\end{bmatrix}\n",
    "= \\mathbf{\\tilde{Y}}_{( \\, s \\, \\times \\, c \\, ) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Network Compnent Relations\n",
    "\n",
    "The input to the network is a group of samples to classify. <br/>\n",
    "The samples are placed into rows of a matrix,\n",
    "$$\n",
    "\\mathbf{X}_{( \\, s \\, \\times \\, f \\, ) } =\n",
    "\\begin{bmatrix}\n",
    "     \\text{---} & x^{(0)}    & \\text{---} \\\\\n",
    "                & \\vdots     &            \\\\\n",
    "     \\text{---} & x^{(s-1)}  & \\text{---} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the network $\\mathbf{X}$ and it spits out its predictions,\n",
    "$$\n",
    "\\mathbf{\\tilde{Y}} = \\mathbf{N}( \\, X \\, )\n",
    "$$\n",
    "The networks output predictions, $ \\mathbf{\\tilde{Y}} $, are bundled just like its input ( with a corresponding sample in every row ),\n",
    "$$\n",
    "\\mathbf{\\tilde{Y}}_{( \\, s \\, \\times \\, c \\, ) } =\n",
    " \\begin{bmatrix}\n",
    "         \\text{---} & \\tilde{y}^{(0)}    & \\text{---} \\\\\n",
    "                    & \\vdots     &            \\\\\n",
    "         \\text{---} & \\tilde{y}^{(s-1)}  & \\text{---} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Each column of its output represents a class and each element a score, <br/>\n",
    "$$\n",
    "\\mathbf{\\tilde{Y}}_{( \\, s \\, \\times \\, c \\, ) } = \n",
    "\\begin{bmatrix}\n",
    "    \\vert             &        & \\vert       \\\\\n",
    "    score_{class_0}   & \\ldots & score_{class_{c-1}} \\\\\n",
    "    \\vert             &        & \\vert\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Now the Networks prediction for $\\mathbf{X}[i,:]$ is its output $\\mathbf{Y}[:,i]$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "x =\n",
    "\\begin{bmatrix} f_0, & \\, \\ldots & \\, f_{n-1} \\end{bmatrix}  \\hspace{1cm}\n",
    "\\rightarrow\n",
    "\\mathbf{\\text{NETWORK}}\n",
    "\\rightarrow  \\hspace{1cm}\n",
    "\\begin{bmatrix}\n",
    "       score_{c_{0}} \\\\\n",
    "       \\vdots \\\\\n",
    "       score_{c_{p-1}}\n",
    "\\end{bmatrix}\n",
    "= \\tilde{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network internals for feed forward process ( inside $ \\mathbf{N}( \\, X \\, ) = \\mathbf{\\tilde{Y}} \\\\ $ ), \n",
    "#### We put $\\mathbf{X}$ through every layer of the network in the same manner. It travels along as activations, $\\mathbf{A}^{(l)}$ and $\\mathbf{Z}^{(l)}$ until finally spit out as $\\mathbf{\\tilde{Y}}$. But it keeps the same structure ( rows of a matrix ) only its columns may be squeezed or streched as it feeds forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{X}_{( \\, s \\, \\times \\, f \\, ) } =\n",
    "\\begin{bmatrix}\n",
    "     \\text{---} & x^{(0)}    & \\text{---} \\\\\n",
    "                & \\vdots     &            \\\\\n",
    "     \\text{---} & x^{(s-1)}  & \\text{---} \\\\\n",
    "\\end{bmatrix}, \\; \\;\n",
    "\\mathbf{A}^{(l)}_{( \\, s \\, \\times \\, in^{(l)} \\, ) } =\n",
    "    \\begin{bmatrix}\n",
    "         \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
    "                    & \\vdots     &            \\\\\n",
    "         \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
    "    \\end{bmatrix}, \\; \\;\n",
    "\\mathbf{Z}^{(l)}_{( \\, s \\, \\times \\, out^{(l)} \\, ) } =\n",
    "    \\begin{bmatrix}\n",
    "         \\text{---} & z^{(0)}    & \\text{---} \\\\\n",
    "                    & \\vdots     &            \\\\\n",
    "         \\text{---} & z^{(s-1)}  & \\text{---} \\\\\n",
    "    \\end{bmatrix}, \\; \\; \\\\\n",
    "\\mathbf{\\tilde{Y}}_{( \\, s \\, \\times \\, c \\, ) } =\n",
    "\\begin{bmatrix}\n",
    "         \\text{---} & \\tilde{y}^{(0)}    & \\text{---} \\\\\n",
    "                    & \\vdots     &            \\\\\n",
    "         \\text{---} & \\tilde{y}^{(s-1)}  & \\text{---} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <center> ( Notice that **every** matrix above has the same number of **rows**, $\\mathbf{s}$, one for each sample ) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heres how it feeds forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\\\\n",
    "\\text{ Step 1. (first step) }\\\\\n",
    "\\mathbf{A}^{(0)} = \\mathbf{X}\n",
    "$$\n",
    "<br/>\n",
    "$$\n",
    "\\\\\n",
    "\\text{ Step 2. } \\\\\n",
    "\\mathbf{A}^{(l+1)} = layer^{(l)}( \\, \\mathbf{A}^{(l)} \\, ) \\; \\; \\text{  for } l = 0, 1, ..., \\mathbf{L}\n",
    "$$\n",
    "<br/>\n",
    "$$\n",
    "\\\\\n",
    "\\text{ Step 3. (final step) } \\\\\n",
    "\\mathbf{\\tilde{Y}} = \\mathbf{A}^{(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inside the $layer^{(l)}$ functions,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{A}^{(l+1)} = layer^{(l)}( \\, \\mathbf{A}^{(l)} \\, ) $$ \n",
    "$$ layer^{(l)}( \\, \\mathbf{A}^{(l)} \\, ) =  \\phi^{(l)} ( \\ \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{where,}\\\\$$\n",
    "$$\n",
    "\\mathbf{A'}^{(l)}\\\\\n",
    "\\text{is just $\\mathbf{A}^{(l)}$ passed through the function,} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~~ \n",
    "Ab[l] = add_bias_unit( A[l] )\n",
    "~~~~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{which is just adding a bias column so we can draw curved lines to seperate the data.} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \n",
       "       \n",
       "               \\text{Takes as input paramater a matrix $\\mathbf{A}_{s \\times n}$,} \\\\\n",
       "               \\text{ }\\\\\n",
       "               \\mathbf{A}_{ input } =\n",
       "                \\begin{bmatrix}\n",
       "                     \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
       "                                & \\vdots     &            \\\\\n",
       "                     \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
       "                \\end{bmatrix}\n",
       "                \\text{ }\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\text{Where '$s$' is the number of instances being fed to the network.}\\\\\n",
       "                \\\\\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\text{Adds a bias column to the first column of $\\mathbf{A}$ and returns the new matrix $\\mathbf{A'}_{s \\times n+1}$, }\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\mathbf{A'}_{column \\, bias} =\n",
       "                \\begin{bmatrix}\n",
       "                     1     &  \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
       "                   \\       &             & \\vdots     &            \\\\\n",
       "                     1     &  \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
       "                \\end{bmatrix}\n",
       "                \n",
       "       $$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calling the functions doc string\n",
    "latex_doc(add_bias_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we shove $ \\mathbf{A'}^{(l)} $ through the following processes to spit out  $ \\mathbf{A'}^{(l+1)} $,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{ Step 1. (first step) } $$\n",
    "$$\n",
    "\\mathbf{Z}^{(l)} = \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \n",
    "$$\n",
    "$$ \\text{ Step 2. (last step) } $$\n",
    "$$\n",
    "\\mathbf{A}^{(l+1)} = \\phi ( \\, \\mathbf{Z}^{(l)} \\, ) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathbf{W}^{(l)}$ is just a matrix of weights. We can think of the first row as bias terms because we added that column of ones to $\\mathbf{A}^{(l)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A'}^{(l)}_{( \\, s \\, \\times \\, in^{(l)}+1 \\, ) } =\n",
    "    \\begin{bmatrix} \n",
    "            1       & \\text{---} &  a^{(0)}   & \\text{---} \\\\\n",
    "           \\vdots   &            & \\ldots     &            \\\\\n",
    "            1       & \\text{---} &  a^{(s-1)} & \\text{---} \\\\\n",
    "    \\end{bmatrix}, \\; \\;\n",
    "\\mathbf{W}^{(l)}_{( \\, in^{(l)}+1 \\, \\times \\, out^{(l)} \\, ) } =\n",
    "   \\begin{bmatrix}\n",
    "        b_{0}   & \\ldots  & b_{out^{(l)}-1} \\\\\n",
    "        w_{1,0} & \\ldots  & w_{1, \\, out^{(l)} -1} \\\\\n",
    "        \\vdots  & \\ldots  & \\vdots \\\\\n",
    "        w_{in^{(l)},0} & \\ldots  & w_{in^{(l)}, \\, out^{(l)} -1 }\n",
    "   \\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z}^{(l)}_{( \\, s \\, \\times \\, out^{(l)} \\, ) } = \\mathbf{A'}^{(l)}_{( \\, s \\, \\times \\, in^{(l)}+1 \\, ) } \\mathbf{W}^{(l)}_{( \\, in^{(l)}+1 \\, \\times \\, out^{(l)} \\, ) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we apply some kind of activation function, $\\phi^{(l)}(\\,\\mathbf{Z}^{(l)}\\,)$, (preferably something easy to take the derivative of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{A}^{(l+1)} = layer^{(l)}( \\, \\mathbf{A}^{(l)} \\, ) \n",
    "=  \\phi^{(l)} ( \\ \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So heres the feed foward,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\tilde{Y}} = \\mathbf{A}^{(L)} = \\phi^{(L-1)} ( \\, \\mathbf{Z}^{(L-1)} \\, ) = \\phi^{(L-1)} \\, ( \\, \\mathbf{A'}^{(L-1)} \\mathbf{W}^{(L-1)} \\, ) = \\phi^{(L-1)} ( \\, \\phi^{(L-2)} ( \\, \\ldots \\, \\, \\phi^{(1)} \\, ( \\phi^{(0)} ( \\ \\, \\mathbf{A'}^{(0)} \\mathbf{W}^{(0)} \\, ) \\, ) \\, \\ldots \\, ) \\, \\, )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions and network components using code notation\n",
    "~~~~ \n",
    "\n",
    "*let s be the number of instances being fed through the network at once\n",
    " (s should be the only variable parameter of a trained network)\n",
    " \n",
    "*let n_hidden_units be a constant size for the input to all internal layers after layer 0, l = 1, 2,..., n_layers-1\n",
    "\n",
    "#### W's - Total number of weight matrices: n_layers, l = 0,1,..., n_layers-1\n",
    "Ws[0].shape is ( n_features+1 x n_hidden_units-1 ) for l = 0\n",
    "W[l].shape is  ( n_hidden_units x n_hidden_units-1 ) for l = 1,..., n_layers-2\n",
    "W[-1].shape is ( n_hidden_units x n_classes ) for l = n_layers-1\n",
    "\n",
    "# A's - total number of A's (shit propigating through network): n_layers+1, l = 0, 1, ..., n_layers-1, n_layers\n",
    "As[0].shape is ( s x n_features ) for l = 0\n",
    "A[l].shape  is ( s x n_hidden_units-1 ) for l = 1, ..., n_layers-1\n",
    "A[-1].shape is ( s x n_classes ) for l = n_layers\n",
    "* note: Y_hat = A[-1]\n",
    "\n",
    "# A_b's - bias column added at index 0 - total number of A_b's: n_layers, l = 0, ..., n_layers-1\n",
    "Abs[0].shape is ( s x n_features+1 ) for l = 0\n",
    "Abs[l].shape is ( s x n_hidden_units ) for l = 1, ..., n_layers-1\n",
    "* note: len(Abs) == len(As)-1\n",
    "\n",
    "# Z's - Z = A_prime @ W - total number of Z's: n_layers, l = 0, ..., n_layers-1\n",
    "Zs[l].shape  is ( s x n_hidden_units-1 ) for l = 0, ..., n_layers-2\n",
    "Zs[-1].shape is ( s x n_classes ) for l = n_layers-1\n",
    "\n",
    "# Y_hat = phi(Zs[-1]) = As[-1] - total number: 1.\n",
    "Y_hat.shape is ( s x n_classes )\n",
    "~~~~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propigation\n",
    "\n",
    "### Finding the optimal $\\mathbf{W}$'s for accurate classifications (Training the network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.\n",
    "* Make a cost function for our weight matrices, $\\mathbf{W}^{(0)}, ..., \\mathbf{W}^{(L-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{J}(\\, \\mathbf{W}^{(0)}, ..., \\mathbf{W}^{(L-1)}, \\, \\mathbf{Y}, \\, \\mathbf{X} \\,)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.\n",
    "* Find the set of  $\\mathbf{W}$'s that makes this the smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can be done iterativley by finding the gradient of the cost function with respect to each $\\mathbf{W}^{(l)}$ and updating them like this,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{W}^{(l)}_{new} \\leftarrow \\mathbf{W}^{(l)}_{old} + \\eta \\, \\mathbf{\\nabla_{W^{(l)}}} \\mathbf{J}(\\, \\mathbf{W}^{(0)}, ..., \\mathbf{W}^{(L-1)}, \\, \\mathbf{Y}, \\, \\mathbf{X} \\,)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{where eta, $\\eta$, is called the learning rate}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~~ \n",
    "# the learning rate in code\n",
    " eta\n",
    "~~~~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding this thing,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\nabla_{W^{(l)}}} \\mathbf{J}(\\, \\mathbf{W}^{(0)}, ..., \\mathbf{W}^{(L-1)}, \\, \\mathbf{Y}, \\, \\mathbf{X} \\,)\\\\\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{ Maybe write the gradient with repsect to $\\mathbf{W}^{(l)}$ like this instead, } \\\\\n",
    "\\mathbf{\\nabla_{W^{(l)}}} \\mathbf{J}(\\, \\mathbf{W}^{(L-1)}, ..., \\mathbf{W}^{(l)}, \\, \\mathbf{Y}, \\, \\mathbf{X} \\,) =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial \\phi^{(L-1)} ( \\, \\mathbf{A'}^{(L-1)} \\mathbf{W}^{(L-1)} \\, )} \\times \n",
    "\\frac{\\partial \\phi^{(L-1)}( \\, \\mathbf{A'}^{(L-1)} \\mathbf{W}^{(L-1)} \\, ) \\, }{\\partial \\phi^{(L-2)} ( \\, \\mathbf{A'}^{(L-2)} \\mathbf{W}^{(L-2)} \\, )\\,} \n",
    "\\times\n",
    "\\ldots\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(l+1)}( \\, \\mathbf{A'}^{(l+1)} \\mathbf{W}^{(l+1)} \\, ) }{\\partial \\phi^{(l)} ( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, )}\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) }{\\partial  \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} } \n",
    "\\times\n",
    "\\frac{\\partial \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} }{\\partial \\mathbf{W}^{(l)} } =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial \\phi^{(L-1)} ( \\, \\mathbf{Z}^{(L-1)} \\, )} \\times \n",
    "\\frac{\\partial \\phi^{(L-1)}( \\, \\mathbf{Z}^{(L-1)} \\, ) \\, }{\\partial \\phi^{(L-2)} ( \\, \\mathbf{Z}^{(L-2)} \\, )\\,} \n",
    "\\times\n",
    "\\ldots\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(l+1)}( \\, \\mathbf{Z}^{(l+1)} \\, ) }{\\partial \\phi^{(l)} ( \\, \\mathbf{Z}^{(l)} \\, )}\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{Z}^{(l)} \\, ) }{\\partial  \\mathbf{Z}^{(l)} } \n",
    "\\times\n",
    "\\frac{\\partial \\mathbf{Z}^{(l)} }{\\partial \\mathbf{W}^{(l)} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{where the derivative of the $l^{th}$ $\\phi$ function with respect to its input weight matrix $\\mathbf{W^{(l)}}$ is, }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) }{\\partial \\mathbf{W}^{(l)} } =\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) \\, }{\\partial \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, } \n",
    "\\times\n",
    "\\frac{\\partial \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, }{\\partial \\mathbf{W}^{(l)} } =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{Z}^{(l)} \\, ) }{\\partial \\mathbf{W}^{(l)} } =\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{Z}^{(l)} \\, ) \\, }{\\partial \\, \\mathbf{Z}^{(l)} \\, } \n",
    "\\times\n",
    "\\frac{\\partial \\, \\mathbf{Z}^{(l)} \\, }{\\partial \\mathbf{W}^{(l)} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{ Where, } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} }{\\partial \\mathbf{W}^{(l)} } = \n",
    "\\mathbf{A'}^{(l)} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{Z}^{(l)} }{\\partial \\mathbf{W}^{(l)} } = \n",
    "\\mathbf{A'}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{ So the derivative of the $l^{th}$ $\\phi$ function with respect to its input weight matrix $\\mathbf{W^{(l)}}$ is,} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{Z}^{(1)} \\, ) \\, }{\\partial \\mathbf{W}^{(l)} } =\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{Z}^{(1)} \\, ) \\, }{\\partial \\mathbf{Z}^{(l)} } \n",
    "\\times\n",
    "\\mathbf{A'}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{ or, }\\\\\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) }{\\partial \\mathbf{W}^{(l)} } =\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) \\, }{\\partial \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, } \n",
    "\\times\n",
    "\\mathbf{A'}^{(l)} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we can write the gradient of the cost function with respect to a specific weight matrix like this,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{ Making the gradient with repsect to $\\mathbf{W}^{(l)}$, } \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{\\nabla_{W^{(l)}}} \\mathbf{J}(\\, \\mathbf{W}^{(L-1)}, ..., \\mathbf{W}^{(l)}, \\, \\mathbf{Y}, \\, \\mathbf{X} \\,) =$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial \\phi^{(L-1)} ( \\, \\mathbf{Z}^{(L-1)} \\, )} \\times \n",
    "\\frac{\\partial \\phi^{(L-1)}( \\, \\mathbf{Z}^{(L-1)} \\, ) \\, }{\\partial \\phi^{(L-2)} ( \\, \\mathbf{Z}^{(L-2)} \\, )\\,} \n",
    "\\times\n",
    "\\ldots\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(l+1)}( \\, \\mathbf{Z}^{(l+1)} \\, ) }{\\partial \\phi^{(l)} ( \\, \\mathbf{Z}^{(l)} \\, )}\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{Z}^{(l)} \\, ) }{\\partial  \\mathbf{Z}^{(l)} } \n",
    "\\times\n",
    "\\mathbf{A'}^{(l)}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This comes directly from this,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\tilde{Y}} = \\phi^{(L-1)} ( \\, \\phi^{(L-2)} ( \\, \\ldots \\, \\, \\phi^{(l+1)} \\, ( \\phi^{(l)} ( \\ \\, \\mathbf{Z}^{(l)} \\, ) \\, ) \\, \\ldots \\, ) \\, \\, )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{or, } \\\\\n",
    "\\mathbf{\\tilde{Y}} = \\phi^{(L-1)} ( \\, \\phi^{(L-2)} ( \\, \\ldots \\, \\, \\phi^{(l+1)} \\, ( \\phi^{(l)} ( \\ \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) \\, ) \\, \\ldots \\, ) \\, \\, )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the last layer $\\mathbf{W}^{(L-1)}$ this is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\nabla_{W^{(L-1)}}} \\mathbf{J} =\n",
    "\\frac{\\partial J}{\\partial \\phi^{(L-1)} ( \\, \\mathbf{Z}^{(L-1)} \\, )} \\times \n",
    "\\frac{\\partial \\phi^{(L-1)}( \\, \\mathbf{Z}^{(L-1)} \\, ) \\, }{\\partial \\mathbf{Z}^{(L-1)}} \n",
    "\\times\n",
    "\\mathbf{A'}^{(L-1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{ or, } \\\\\n",
    "\\mathbf{\\nabla_{W^{(L-1)}}} \\mathbf{J} =\n",
    "\\frac{\\partial J}{\\partial \\phi^{(L-1)} ( \\, \\mathbf{A'}^{(L-1)} \\mathbf{W}^{(L-1)} \\, )} \\times \n",
    "\\frac{\\partial \\phi^{(L-1)}( \\, \\mathbf{A'}^{(L-1)} \\mathbf{W}^{(L-1)} \\, ) \\, }{\\partial \\mathbf{A'}^{(L-1)} \\mathbf{W}^{(L-1)}} \n",
    "\\times\n",
    "\\mathbf{A'}^{(L-1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a MSE cost function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{J} = \\frac{( \\mathbf{Y} - \\mathbf{\\tilde{Y}} )^{2}}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient for $\\mathbf{W^{(l)}}$ is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{\\nabla_{W^{(l)}}} \\mathbf{J} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\nabla_{W^{(L-1)}}} \\mathbf{J} =\n",
    "( \\mathbf{Y} - \\mathbf{\\tilde{Y}} )\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(L-1)}( \\, \\mathbf{Z}^{(L-1)} \\, ) \\, }{\\partial \\phi^{(L-2)} ( \\, \\mathbf{Z}^{(L-2)} \\, )\\,} \n",
    "\\times\n",
    "\\ldots\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(l+1)}( \\, \\mathbf{Z}^{(l+1)} \\, ) }{\\partial \\phi^{(l)} ( \\, \\mathbf{Z}^{(l)} \\, )}\n",
    "\\times\n",
    "\\frac{\\partial \\phi^{(l)}( \\, \\mathbf{Z}^{(l)} \\, ) }{\\partial  \\mathbf{Z}^{(l)} } \n",
    "\\times\n",
    "\\mathbf{A'}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\mathbf{\\tilde{Y}} = \\mathbf{A}^{(L)} = \\phi^{(L-1)} ( \\ \\, \\mathbf{Z}^{(L-1)} \\, )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a sigmoid $\\phi$ function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\phi ( g ) }{\\partial g } = \\phi ( g ) * ( \\, 1 \\, -  \\, \\phi ( \\, g \\, ) \\, )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\phi^{(l)} ( \\, \\mathbf{Z}^{(l)} \\, ) }{\\partial \\, \\mathbf{Z}^{(l)} \\, } \n",
    "=  \n",
    "\\phi ( \\, \\mathbf{Z}^{(l)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi ( \\, \\mathbf{Z}^{(l)} \\, ) \\, )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\phi^{(l)} ( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) }{\\partial \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, } \n",
    "=  \n",
    "\\phi ( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi ( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) \\, )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\phi^{(l)} ( \\, \\mathbf{Z'}^{(l)} \\, ) }{\\partial \\ \\mathbf{W}^{(l)} \\, } =  ( \\phi ( \\, \\mathbf{Z}^{(l)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi( \\, \\mathbf{Z}^{(l)} \\, ) \\, ) \\, )\\; \\mathbf{A'}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\phi^{(l)} ( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) }{\\partial \\ \\mathbf{W}^{(l)} \\, } \n",
    "=  \n",
    "( \\phi ( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi ( \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) \\, ) ) \\; \\mathbf{A'}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So for MSE and sigmoid\n",
    "### the cost for  $\\mathbf{W}^{(l)}$ would be,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{\\nabla_{W^{(l)}}} \\mathbf{J} = \\\\\n",
    "( \\mathbf{Y} - \\mathbf{\\tilde{Y}} ) \\;\n",
    "\\times\n",
    "( \\, \\phi ( \\, \\mathbf{Z'}^{(L-1)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi( \\, \\mathbf{Z'}^{(L-1)} \\, ) \\, ) \\, )\\; \n",
    "\\times\n",
    "( \\, \\phi ( \\, \\mathbf{Z'}^{(L-2)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi( \\, \\mathbf{Z'}^{(L-2)} \\, ) \\, ) \\; \\;\n",
    "\\times \\ldots \\; \\\\\n",
    "\\ldots \\;\n",
    "\\times\n",
    "( \\, \\phi ( \\, \\mathbf{Z'}^{(l+1)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi( \\, \\mathbf{Z'}^{(l+1)} \\, ) \\, ) \\, )\\;\n",
    "\\times\n",
    "( \\, \\phi ( \\, \\mathbf{Z'}^{(l)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi( \\, \\mathbf{Z'}^{(l)} \\, ) \\, ) \\, )\\;\n",
    "\\mathbf{A'}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So for MSE and sigmoid\n",
    "### the cost for  $\\mathbf{W}^{(L-1)}$ would be,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{\\nabla_{W^{(L-1)}}} \\mathbf{J} =\n",
    "( \\mathbf{Y} - \\mathbf{\\tilde{Y}} ) \\;\n",
    "\\times\n",
    "( \\, \\phi ( \\, \\mathbf{Z'}^{(L-1)} \\, ) \\, * ( \\, 1 \\, -  \\, \\phi( \\, \\mathbf{Z'}^{(L-1)} \\, ) \\, ) \\;\n",
    "\\times\n",
    "\\mathbf{A'}^{(L-1)} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{\\nabla_{W^{(L-1)}}} \\mathbf{J} =\n",
    "( \\mathbf{Y} - \\mathbf{\\tilde{Y}} ) \\;\n",
    "\\times\n",
    "( \\,  \\mathbf{\\tilde{Y}} \\, * ( \\, 1 \\, -  \\,  \\mathbf{\\tilde{Y}} \\, ) \\, ) \\;\n",
    "\\times\n",
    "\\mathbf{A'}^{(L-1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class layer:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, phi, dphi):\n",
    "        \"\"\"\n",
    "            phi is an activation function to apply element wise\n",
    "            dphi is its derivative\n",
    "        \"\"\"\n",
    "        self.n_input = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation_function = phi\n",
    "        \n",
    "        # initilize weights, adding an additional row for its biases at index 0\n",
    "        self.W = build_weight_matrix( self.n_input+1, self.n_outputs )\n",
    "        \n",
    "    def activate(self, input):\n",
    "        # stored inputs as A\n",
    "        self.A = input\n",
    "        \n",
    "        # add bias column to input of 1s at index 0\n",
    "        self.Ab = add_bias_unit(self.A)\n",
    "        \n",
    "        # compute biased_input @ W\n",
    "        self.Z = self.Ab @ self.W\n",
    "        \n",
    "        # apply phi activation function\n",
    "        self.output = self.phi(self.Z)\n",
    "        \n",
    "        return self.output\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class MLPBase(object):\n",
    "    \n",
    "    def __init__(self, n_layers, n_hidden_units=30, C=0.0, epochs=500, eta=0.001, phi=\"sigmoid\", objective_function=\"quadratic\", random_state=None):\n",
    "        \n",
    "        # [TODO]: reproduce results by initilizing W with the same weights\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # number of  layers for the network\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # number of units for each hidden layer\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        \n",
    "        # lambda value for l2 regularization\n",
    "        self.l2_C = C\n",
    "        \n",
    "        # maximum training epochs\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # activation function\n",
    "        self.phi = phi\n",
    "        \n",
    "        # objection function\n",
    "        self.objective_function = objective_function\n",
    "\n",
    "    def _initilize_weights(self):\n",
    "        \"\"\"\n",
    "            Build and initilize weight matrices for network.\n",
    "            \n",
    "            W's - Total number of weight matrices: n_layers, l = 0,1,..., n_layers-1\n",
    "            \n",
    "            Ws[0].shape is ( n_features+1 x n_hidden_units-1 ) for l = 0\n",
    "            W[l].shape is  ( n_hidden_units x n_hidden_units-1 ) for l = 1,..., n_layers-2\n",
    "            W[-1].shape is ( n_hidden_units x *n_classes* ) for l = n_layers-1\n",
    "            \n",
    "            Initialize weights with small random numbers from uniform distribution range [-1,1]\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # tmp***\n",
    "        assert self.n_layers > 2\n",
    "        \n",
    "        #  Ws[0].shape is ( n_features+1 x n_hidden_units-1 )\n",
    "        W_first = build_weight_matrix( self.n_features+1 , self.n_hidden_units-1 )\n",
    "        \n",
    "        #  W[l].shape is ( n_hidden_units x n_hidden_units -1 ) for l = 1,..., n_layers-2\n",
    "        interior_Ws = [build_weight_matrix( self.n_hidden_units , self.n_hidden_units-1 ) for i in range(0,self.n_layers-2)]\n",
    "        \n",
    "        #  W[-1].shape is ( n_hidden_units x n_classes )\n",
    "        W_last = build_weight_matrix( self.n_hidden_units , self.n_classes )\n",
    "        \n",
    "        # store initilized weight matrices\n",
    "        self.Ws = [W_first] + interior_Ws + [W_last]\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y) #.values.T\n",
    "        return onehot\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear(z):\n",
    "        return z\n",
    "    \n",
    "    def _phi(self, z):\n",
    "        if self.phi == \"sigmoid\":\n",
    "            return self._sigmoid(z)\n",
    "        if self.phi == \"linear\":\n",
    "            return self._linear(z)\n",
    "        \n",
    "    def _phi_grad(self, Z):\n",
    "        if self.phi == \"sigmoid\":\n",
    "            sigmoid_derivative = self._sigmoid(Z) * (1. - self._sigmoid(Z))\n",
    "            return sigmoid_derivative\n",
    "        \n",
    "        if self.phi == \"linear\":\n",
    "            return Z\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _objective_grad(self, y, y_hat):\n",
    "        obj_grad = -2*(y - y_hat) * self._phi_grad(y_hat)\n",
    "        return obj_grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def _MSE(Y_enc, Y_hat):\n",
    "        cost = np.mean((Y_enc-Y_hat)**2)\n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def _log_likelihood(Y_enc, Y_hat):\n",
    "        cost = np.sum(-(Y_enc * (np.log(Y_hat))))\n",
    "        return cost\n",
    "        \n",
    "    def _cost(self,Y_hat,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        if self.objective_function == \"quadratic\":\n",
    "            cost = self._MSE(Y_enc, Y_hat)\n",
    "        if self.objective_function == \"cross_entropy\":\n",
    "            cost = self._log_likelihood(Y_enc, Y_hat)\n",
    "        #cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(MLPBase):\n",
    "    \n",
    "    def _feedforward(self, X):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        \n",
    "        # store A's Ab's and Z's\n",
    "        As = []\n",
    "        Abs = []\n",
    "        Zs = []\n",
    "        \n",
    "        # set A0 = X and begin feed forward process\n",
    "        A0 = X\n",
    "        As += [A0]\n",
    "        \n",
    "        for W in self.Ws:\n",
    "            \n",
    "            # add a bias column to the last A\n",
    "            Ab = add_bias_unit( As[-1] )\n",
    "            \n",
    "            # store\n",
    "            Abs += [Ab]\n",
    "            \n",
    "            # calculate Z\n",
    "            Z = Ab @ W\n",
    "            \n",
    "            # store\n",
    "            Zs += [Z]\n",
    "            \n",
    "            # compute next A by\n",
    "            # applying activation function\n",
    "            A_next = self._phi(Z)\n",
    "            \n",
    "            # store\n",
    "            As += [A_next]\n",
    "            \n",
    "        # As should have one extra element then the other two lists\n",
    "        # this is the output of our network before one hot encoding\n",
    "        # Y_hat = As[-1]\n",
    "        \n",
    "        return As, Abs, Zs\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        # copy data\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        \n",
    "        # setup one hot encoding\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_classes = Y_enc.shape[1]\n",
    "        self._initilize_weights()\n",
    "        \n",
    "        As, Abs, Zs = self._feedforward(X_data)\n",
    "        \n",
    "        i = 0\n",
    "        for A, Ab, W, Z in zip(As, Abs, self.Ws, Zs):\n",
    "            print(\"A_%s shape: %s\" % (i, A.shape))\n",
    "            print(\"Ab_%s shape: %s\" % (i, Ab.shape))\n",
    "            print(\"W_%s shape: %s\" % (i, W.shape))\n",
    "            print(\"Z_%s shape: %s\" % (i, Z.shape))\n",
    "            i+=1\n",
    "        \n",
    "        #\n",
    "        #\n",
    "        # Backprop\n",
    "        #\n",
    "        #\n",
    "        \n",
    "        # let Y_hat = As[-1]\n",
    "        Y_hat = As[-1]\n",
    "  \n",
    "        output_error = Y_enc - Y_hat\n",
    "        output_delta = output_error * self._phi_grad(Y_hat)  # dJ_dYhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_layers=4,\n",
    "              n_hidden_units=30, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=200, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              phi=\"sigmoid\",\n",
    "              objective_function=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_0 shape: (1437, 64)\n",
      "Ab_0 shape: (1437, 65)\n",
      "W_0 shape: (65, 29)\n",
      "Z_0 shape: (1437, 29)\n",
      "A_1 shape: (1437, 29)\n",
      "Ab_1 shape: (1437, 30)\n",
      "W_1 shape: (30, 29)\n",
      "Z_1 shape: (1437, 29)\n",
      "A_2 shape: (1437, 29)\n",
      "Ab_2 shape: (1437, 30)\n",
      "W_2 shape: (30, 29)\n",
      "Z_2 shape: (1437, 29)\n",
      "A_3 shape: (1437, 29)\n",
      "Ab_3 shape: (1437, 30)\n",
      "W_3 shape: (30, 10)\n",
      "Z_3 shape: (1437, 10)\n",
      "CPU times: user 30.8 ms, sys: 11.2 ms, total: 42 ms\n",
      "Wall time: 14.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = MLP(**params)\n",
    "nn.fit(X_train, y_train, print_progress=10)\n",
    "#yhat = nn.predict(X_test)\n",
    "#print('Test acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some formulas to remeber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z}^{(l)} = \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \n",
    "$$\n",
    "$$\n",
    "\\mathbf{A}^{(l+1)} = \\phi ( \\, \\mathbf{Z}^{(l)} \\, ) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{A}^{(l+1)} = \\phi^{(l)} ( \\ \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{Z}^{(l+1)} = \\phi^{(l)} ( \\ \\, \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} \\, ) \\mathbf{W}^{(l+1)} =$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{Z}^{(l+1)} = \\phi^{(l)} ( \\ \\, \\mathbf{Z}^{(l)} \\, ) \\mathbf{W}^{(l+1)} = \\mathbf{A'}^{(l)} \\mathbf{W}^{(l+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{A'}^{(l)} \\mathbf{W}^{(l)} }{\\partial \\mathbf{W}^{(l)} } = \n",
    "\\mathbf{A'}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{Z}^{(l)} }{\\partial \\mathbf{W}^{(l)} } = \n",
    "\\mathbf{A'}^{(l)}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
