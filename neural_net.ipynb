{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "-0.5 0.5\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# lets load up the handwritten digit dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "digit = 4\n",
    "x_digits = X[y == digit]\n",
    "for i in range(25):\n",
    "    img = x_digits[i].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(1437,)\n",
      "(360, 64)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network as a black box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the network an **instance vector**, $x$, we would like to send it through our trained **network** and have the newtork output it's prediction for the **class** $x$ belongs to. This networks output prediction comes in the form of a **prediction vector**, $\\tilde{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "x\n",
    "\\hspace{1cm}\n",
    "\\rightarrow\n",
    "\\mathbf{\\text{NETWORK}}\n",
    "\\rightarrow  \\hspace{1cm}\n",
    "\\tilde{y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance vector's elements are called **features**, and the network uses these features to determine the class that particular instance most likley belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say our instances all have $n$ features $\\left( \\, \\, f_0, \\, f_1, \\, \\ldots, \\, f_{n-1} \\, \\right)$, <br/> and we want to classify these instances into one of $p$ classes $\\left( \\, c_0, \\, c_1, \\, \\ldots, \\, c_{p-1} \\, \\right)$.  <br/>\n",
    "Then we write the instance vectors and prediction vectors like this,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "x =\n",
    "\\begin{bmatrix} f_{0}, & \\, \\ldots, & \\, f_{n-1} \\end{bmatrix},\n",
    "\\hspace{1cm}\n",
    "\\begin{bmatrix}\n",
    "       score_{c_{0}} \\\\\n",
    "       \\vdots \\\\\n",
    "       score{c_{p-1}}\n",
    "\\end{bmatrix}\n",
    "= \\tilde{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending the instance $x$ through our network results in the networks prediction vector $y$ with a score for each class in it's assosciated row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "x =\n",
    "\\begin{bmatrix} f_0, & \\, \\ldots & \\, f_{n-1} \\end{bmatrix}  \\hspace{1cm}\n",
    "\\rightarrow\n",
    "\\mathbf{\\text{NETWORK}}\n",
    "\\rightarrow  \\hspace{1cm}\n",
    "\\begin{bmatrix}\n",
    "       score_{c_{0}} \\\\\n",
    "       \\vdots \\\\\n",
    "       score_{c_{p-1}}\n",
    "\\end{bmatrix}\n",
    "= \\tilde{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row with largest value in $ \\tilde{y} $ is usually taken to be the predicted class. \n",
    "So for the input vector, $x$, the predicted class would be the class corresponding to the row in the prediction vector, $y$, with the largest value. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say the $j^{th}$ row is the largest value in $\\tilde{y}$, then the class corresponding to row $j$ ($c_j$) is the networks perdiction for the instance $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{y} =\n",
    "\\begin{bmatrix}\n",
    "       score_{c_{0}} \\\\\n",
    "       \\vdots \\\\\n",
    "       \\mathbf{score_{c_j}}\\\\\n",
    "       \\vdots \\\\\n",
    "       score_{c_{p-1}}\n",
    "\\end{bmatrix}, \\text{ where } score_{c_j} \\text{ is the } max \\left( score \\right) \\in  \\tilde{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the network predicted that the instance $x$ is a memeber of class $c_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at what this might look like using some python code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import numpy as np`<br/><br/>\n",
    "\n",
    "`# let's use a network that takes instances with 3 features`<br/>\n",
    "`f0, f1, f2 = 1, 1, 2`<br/><br/>\n",
    "\n",
    "`# and lets say the network has two classes`<br/>\n",
    "`class_dictonary = {0:'class_0', 1:'class_1'}`<br/><br/>\n",
    "\n",
    "`# create the instance vector x that we would like to classify...`<br/>\n",
    "`x = np.array([f0,f1,f2])` <br/><br/>\n",
    "\n",
    "`# pass the instance to our trained network for its prediction...`<br/>\n",
    "`y_hat = network.predict(x)` <br/><br/>\n",
    "\n",
    "`# printing y_hat we will see the scores our network gave for each class`<br/>\n",
    "`print(y_hat)`<br/>\n",
    "`  [0.9 0.2]`<br/><br/>\n",
    "\n",
    "`# get the index of y_hat with the highest score`<br/>\n",
    "`predicted_class_index = np.argmax( y_hat )`<br/><br/>\n",
    "\n",
    "`# and use the class dictonary to make it human readable`<br/>\n",
    "`prediction = class_dictonary[ predicted_class_index ]`<br/><br/>\n",
    "\n",
    "`print( prediction )`<br/>\n",
    "`  class_0`<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Matrices\n",
    "With lots of instances it becomes useful to group these vectors into Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are $m$ instance vectors, $ \\left( \\, x_{0}, \\, \\ldots, \\, x_{m-1} \\, \\right) $, each with $n$ features, <br/>\n",
    "\n",
    "$$ \n",
    "x_i =\n",
    "\\begin{bmatrix} f_0, & \\, \\ldots, & \\, f_{n-1} \\end{bmatrix},\n",
    "\\hspace{0.5cm} for \\; i = 0, \\, \\ldots, \\, n-1\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can group these instance vectors into rows in an input matrix, $ \\mathbf{X}_{m \\times n} $, and use this matrix as an input to the network, <br/> <br/>\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "    \\text{---} & x_0 & \\text{---} \\\\\n",
    "               & \\ldots    & \\\\\n",
    "    \\text{---} & x_{m-1} & \\text{---}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the output prediction vectors of the network for each instance represented as a matrix of column vectors, <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\tilde{Y}} = \n",
    "\\begin{bmatrix}\n",
    "    \\vert         &        & \\vert       \\\\\n",
    "    \\tilde{y}_0   & \\ldots & \\tilde{y}_{m-1} \\\\\n",
    "    \\vert         &        & \\vert\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of **rows** in $\\mathbf{\\tilde{Y}}$ is equal to the number of **classes** the network is trying to classify and the output of the network would be $\\mathbf{\\tilde{Y}}_{m \\times p}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\tilde{Y}} =\n",
    "\\begin{bmatrix}\n",
    "    \\text{---} & score_{c_0} & \\text{---} \\\\\n",
    "               & \\ldots    & \\\\\n",
    "    \\text{---} & score_{c_{p-1}} & \\text{---}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction for a single instance $x_i$ ( which is the $i^{th}$ row of $\\mathbf{X}$ )\n",
    "would be $ \\tilde{y_i} $, which is the $i^{th}$ column in $\\mathbf{\\tilde{Y}}$. <br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "    \\text{---} & x_0 & \\text{---} \\\\\n",
    "               & \\ldots    & \\\\\n",
    "    \\mathbf{\\text{---}} & \\mathbf{x_i} & \\mathbf{\\text{---}} \\\\\n",
    "               & \\ldots    & \\\\\n",
    "    \\text{---} & x_{m-1} & \\text{---}\n",
    "\\end{bmatrix} \\hspace{1cm}\n",
    "\\rightarrow\n",
    "\\mathbf{\\text{NETWORK}}\n",
    "\\rightarrow \\hspace{1cm}\n",
    "\\begin{bmatrix} \n",
    "    \\vert         &        &\\mathbf{\\text{|}}      &        & \\vert       \\\\\n",
    "    \\tilde{y}_0   & \\ldots &\\mathbf{\\tilde{y}_i}   & \\ldots & \\tilde{y}_{m-1} \\\\\n",
    "    \\vert         &        &\\mathbf{\\text{|}}      &        & \\vert\n",
    "\\end{bmatrix}\n",
    "= \\mathbf{\\tilde{Y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Now the Networks prediction for $\\mathbf{X}[i,:]$ is its output $\\mathbf{Y}[:,i]$</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to what we had above,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "x =\n",
    "\\begin{bmatrix} f_0, & \\, \\ldots & \\, f_{n-1} \\end{bmatrix}  \\hspace{1cm}\n",
    "\\rightarrow\n",
    "\\mathbf{\\text{NETWORK}}\n",
    "\\rightarrow  \\hspace{1cm}\n",
    "\\begin{bmatrix}\n",
    "       score_{c_{0}} \\\\\n",
    "       \\vdots \\\\\n",
    "       score_{c_{p-1}}\n",
    "\\end{bmatrix}\n",
    "= \\tilde{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Network Aretecture\n",
    "\"What is happening when the network makes its predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Perceptron\n",
    "Lets start with the simple case of one layer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the single layer case, if we ignore bias terms and activation functions then we are just trying to solve the linear system bellow for $\\mathbf{W}$,\n",
    "\n",
    "$$\n",
    "    \\mathbf{W} \\mathbf{X^{T}} = \\mathbf{Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that $\\mathbf{Y}$ is missing it's hat ( $\\mathbf{\\tilde{Y}}$ )..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our network to predict the classes for each row in $\\mathbf{X}$ correctly, so we \"train\" our network by solving this system for $\\mathbf{W}$ using instances we know the true classes for. The true classes are denoted by dropping the hat on $\\mathbf{Y}$. When we solve this system for $\\mathbf{W}$ on our training data we hope that it will provide accurate predictions for examples we encounter in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the multilayer case lets introduce another variable now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{A} = \\mathbf{X^{T}}$ and the system we are trying to solve is now,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathbf{W} \\mathbf{A} = \\mathbf{Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\text{Pythagorean Theorem} \\\\\n",
       "                a^2 + b^2 = c^2\n",
       "        $$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env\n",
    "\n",
    "__filename__ = \"latex_doc_strings.py\"\n",
    "__author__ = \"L.J. Brown\"\n",
    "\n",
    "import re\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "def extract_doc_latex(method):\n",
    "    \"\"\"\n",
    "        looks for latex in doc strings of passes method.\n",
    "        \n",
    "        Format example for docstring:\n",
    "        \n",
    "            r\\\"\"\" \n",
    "                    Foo Method Description \n",
    "\n",
    "                :latex: MULTI-LINE\n",
    "                        LATEX EXPRESSION\n",
    "                        INSERTED HERE\n",
    "                        \n",
    "                :param bar: (example parameter)\n",
    "                :returns: (whatever) \n",
    "\n",
    "                * [Notes] : \n",
    "                \n",
    "                    - latex code must be followed by \n",
    "                        at least one ':'(like the one \n",
    "                        leading param above)\n",
    "                        \n",
    "                    - doc string must be proceeded by r\\\"\"\" \n",
    "            \\\"\"\"\n",
    "            \n",
    "        :param: method with string literal doc string containing latex\n",
    "        :returns: latex string literal\n",
    "        \n",
    "    \"\"\"\n",
    "    p = re.compile('(?:\\:latex\\:)([\\s\\S]+?)(?:\\:)', re.MULTILINE)\n",
    "    m = p.search(method.__doc__)\n",
    "    return m.group(1)\n",
    "\n",
    "def latex_doc(method):\n",
    "    \"\"\"\n",
    "    returns rendered latex from doc string of method passed as parameter\n",
    "\n",
    "    Format example for docstring:\n",
    "\n",
    "            r\\\"\"\" \n",
    "                    Foo Method Description \n",
    "\n",
    "                :latex: MULTI-LINE\n",
    "                        LATEX EXPRESSION\n",
    "                        INSERTED HERE\n",
    "\n",
    "                :param bar: (example parameter)\n",
    "                :returns: (whatever) \n",
    "\n",
    "                * [Notes] : \n",
    "\n",
    "                    - latex code must be followed by \n",
    "                        at least one ':'(like the one \n",
    "                        leading param above)\n",
    "\n",
    "                    - doc string must be proceeded by r\\\"\"\" \n",
    "            \\\"\"\"\n",
    "\n",
    "    :param: method with string literal doc string containing latex\n",
    "    :returns: formated latex expression\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    latex_string = extract_doc_latex(method)\n",
    "    return display(Math(latex_string))\n",
    "\n",
    "# example method with latex in doc string formated correctly\n",
    "def foo(bar):\n",
    "    r\"\"\"\n",
    "            Example method with latex in doc string\n",
    "\n",
    "        :latex: \\text{Pythagorean Theorem} \\\\\n",
    "                a^2 + b^2 = c^2\n",
    "        :param bar: example parameter\n",
    "        :returns: None\n",
    "    \"\"\"\n",
    "\n",
    "# render doc string latex for example method above\n",
    "latex_doc(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \n",
       "       \n",
       "               \\text{Takes as input paramater a matrix $M_{m \\times n}$.} \\\\\n",
       "               \\text{Adds a bias row or column of '$1's$' to the first row } \\\\\n",
       "               \\text{or column, depending on input parameter 'how'.} \\\\\n",
       "               \\text{('how' parameter defaults to 'column')}\\\\ \\\\ \\\\\n",
       "               \\mathbf{M_{input}} =\n",
       "                \\begin{bmatrix}\n",
       "                    m_{0,0} & \\ldots  & m_{0,n} \\\\\n",
       "                    \\vdots  & \\ldots  & \\vdots \\\\\n",
       "                    m_{m,0} & \\ldots  & m_{m,n}\n",
       "                \\end{bmatrix}\n",
       "                \\\\\\\\\n",
       "                \\text{Returns, }\\\\\n",
       "                \\mathbf{M'_{column \\, bias}} =\n",
       "                \\begin{bmatrix}\n",
       "                     1     &  m_{0,0} & \\ldots  & m_{0,n+1} \\\\\n",
       "                   \\vdots  &  \\vdots  & \\ldots  & \\vdots \\\\\n",
       "                     1     &  m_{m,0} & \\ldots  & m_{m,n+1}\n",
       "                \\end{bmatrix}, \n",
       "                \\begin{bmatrix}\n",
       "                     1       & \\ldots  & 1      \\\\\n",
       "                     m_{0,0} & \\ldots  & m_{0,n} \\\\\n",
       "                     \\vdots  & \\ldots  & \\vdots  \\\\\n",
       "                     m_{m+1,0} & \\ldots  & m_{m+1,n}\n",
       "                \\end{bmatrix} \n",
       "                = \\mathbf{M'_{row \\, bias}}\n",
       "       $$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_bias(M, how='column'):\n",
    "    r\"\"\"\n",
    "        Add bias unit (column or row of 1s) to array at index 0\n",
    "        \n",
    "       :latex: \n",
    "       \n",
    "               \\text{Takes as input paramater a matrix $M_{m \\times n}$.} \\\\\n",
    "               \\text{Adds a bias row or column of '$1's$' to the first row } \\\\\n",
    "               \\text{or column, depending on input parameter 'how'.} \\\\\n",
    "               \\text{('how' parameter defaults to 'column')}\\\\ \\\\ \\\\\n",
    "               \\mathbf{M_{input}} =\n",
    "                \\begin{bmatrix}\n",
    "                    m_{0,0} & \\ldots  & m_{0,n} \\\\\n",
    "                    \\vdots  & \\ldots  & \\vdots \\\\\n",
    "                    m_{m,0} & \\ldots  & m_{m,n}\n",
    "                \\end{bmatrix}\n",
    "                \\\\\\\\\n",
    "                \\text{Returns, }\\\\\n",
    "                \\mathbf{M'_{column \\, bias}} =\n",
    "                \\begin{bmatrix}\n",
    "                     1     &  m_{0,0} & \\ldots  & m_{0,n+1} \\\\\n",
    "                   \\vdots  &  \\vdots  & \\ldots  & \\vdots \\\\\n",
    "                     1     &  m_{m,0} & \\ldots  & m_{m,n+1}\n",
    "                \\end{bmatrix}, \n",
    "                \\begin{bmatrix}\n",
    "                     1       & \\ldots  & 1      \\\\\n",
    "                     m_{0,0} & \\ldots  & m_{0,n} \\\\\n",
    "                     \\vdots  & \\ldots  & \\vdots  \\\\\n",
    "                     m_{m+1,0} & \\ldots  & m_{m+1,n}\n",
    "                \\end{bmatrix} \n",
    "                = \\mathbf{M'_{row \\, bias}}\n",
    "       :param M: numpy array to add bias row or column to\n",
    "       :param how: 'column' or 'row', default 'coulmn'\n",
    "    \"\"\"\n",
    "    if how == 'column':\n",
    "        ones = np.ones((M.shape[0], 1))\n",
    "        M_new = np.hstack((ones, M))\n",
    "    elif how == 'row':\n",
    "        ones = np.ones((1, M.shape[1]))\n",
    "        M_new = np.vstack((ones, M))\n",
    "    return A_new\n",
    "\n",
    "latex_doc(add_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(M):\n",
    "    \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "    W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "    W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "    W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations and abilities of this system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias terms and phi functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi $ \\Phi( \\; Z \\; ) $  Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single instance input $ \\phi( \\; \\vec{z} \\; ) $ and matrix input $ \\Phi( \\; \\mathbf{Z} \\; ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\vec{a_{i+1}} = \\phi ( \\; \\vec{z_{i}} \\; ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf{A_{i+1}} = \\Phi ( \\; \\mathbf{Z_{i}} \\; ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30, C=0.0, epochs=500, eta=0.001, phi=\"sigmoid\", objective_function=\"quadratic\", random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.phi = phi\n",
    "        self.objective_function = objective_function\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear(z):\n",
    "        return z\n",
    "    \n",
    "    def _phi(self, z):\n",
    "        if self.phi == \"sigmoid\":\n",
    "            return self._sigmoid(z)\n",
    "        if self.phi == \"linear\":\n",
    "            return self._linear(z)\n",
    "        \n",
    "    def _phi_grad(self, a):\n",
    "        if self.phi == \"sigmoid\":\n",
    "            return a*(1-a)\n",
    "        if self.phi == \"linear\":\n",
    "            return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _objective_grad(self, y, y_hat):\n",
    "        obj_grad = -2*(y - y_hat) * self._phi_grad(y_hat)\n",
    "        return obj_grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def _MSE(Y_enc, Y_hat):\n",
    "        cost = np.mean((Y_enc-Y_hat)**2)\n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def _log_likelihood(Y_enc, Y_hat):\n",
    "        #term1 = -Y_enc * (np.log(Y_hat))\n",
    "        #term2 = (1.0 - Y_enc) * np.log(1.0 - Y_hat)\n",
    "        #cost = np.sum(term1 - term2)\n",
    "        cost = np.sum(Y_enc * (np.log(Y_hat)))\n",
    "        return cost\n",
    "        \n",
    "    def _cost(self,Y_hat,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        if self.objective_function == \"quadratic\":\n",
    "            cost = self._MSE(Y_enc, Y_hat)\n",
    "        if self.objective_function == \"cross_entropy\":\n",
    "            cost = self._log_likelihood(Y_enc, Y_hat)\n",
    "        #cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._phi(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._phi(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        grad1 = np.zeros(W1.shape)\n",
    "        grad2 = np.zeros(W2.shape)\n",
    "        \n",
    "        # for each instance's activations \n",
    "        for (a1,a2,a3,y) in zip(A1.T,A2.T,A3.T,Y_enc.T):\n",
    "            \n",
    "            dJ_dz2 = self._objective_grad(y, a3)\n",
    "            dJ_dz1 = dJ_dz2 @ W2 @ np.diag(self._phi_grad(a2))\n",
    "                         \n",
    "            dz2_dw2 = a2[np.newaxis,:]\n",
    "            dz1_dw1 = a1[np.newaxis,:]\n",
    "            \n",
    "            # grad = VA.T \n",
    "            grad2 += dJ_dz2[:,np.newaxis]  @ dz2_dw2\n",
    "            grad1 += dJ_dz1[1:,np.newaxis] @ dz1_dw1\n",
    "            # don't incorporate bias term in the z1 gradient \n",
    "            # (its added in a2 from another layer)\n",
    "            \n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += (W1[:, 1:] * self.l2_C)\n",
    "        grad2[:, 1:] += (W2[:, 1:] * self.l2_C)\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_hidden=50, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=200, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              phi=\"sigmoid\",\n",
    "              objective_function=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:85: RuntimeWarning: overflow encountered in square\n",
      "/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:80: RuntimeWarning: overflow encountered in multiply\n",
      "Epoch: 200/200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.12222222222222222\n",
      "CPU times: user 24.5 s, sys: 2.79 s, total: 27.3 s\n",
      "Wall time: 6.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = TwoLayerPerceptron(**params)\n",
    "nn.fit(X_train, y_train, print_progress=10)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
