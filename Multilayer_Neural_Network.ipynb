{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "-0.5 0.5\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# lets load up the handwritten digit dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "digit = 4\n",
    "x_digits = X[y == digit]\n",
    "for i in range(25):\n",
    "    img = x_digits[i].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(1437,)\n",
      "(360, 64)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env\n",
    "\n",
    "__filename__ = \"latex_doc_strings.py\"\n",
    "__author__ = \"L.J. Brown\"\n",
    "\n",
    "import re\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "def extract_doc_latex(method):\n",
    "    \"\"\"\n",
    "        looks for latex in doc strings of passes method.\n",
    "        \n",
    "        Format example for docstring:\n",
    "        \n",
    "            r\\\"\"\" \n",
    "                    Foo Method Description \n",
    "\n",
    "                :latex: MULTI-LINE\n",
    "                        LATEX EXPRESSION\n",
    "                        INSERTED HERE\n",
    "                        \n",
    "                :param bar: (example parameter)\n",
    "                :returns: (whatever) \n",
    "\n",
    "                * [Notes] : \n",
    "                \n",
    "                    - latex code must be followed by \n",
    "                        at least one ':'(like the one \n",
    "                        leading param above)\n",
    "                        \n",
    "                    - doc string must be proceeded by r\\\"\"\" \n",
    "            \\\"\"\"\n",
    "            \n",
    "        :param: method with string literal doc string containing latex\n",
    "        :returns: latex string literal\n",
    "        \n",
    "    \"\"\"\n",
    "    p = re.compile('(?:\\:latex\\:)([\\s\\S]+?)(?:\\:)', re.MULTILINE)\n",
    "    m = p.search(method.__doc__)\n",
    "    return m.group(1)\n",
    "\n",
    "def latex_doc(method):\n",
    "    \"\"\"\n",
    "    returns rendered latex from doc string of method passed as parameter\n",
    "\n",
    "    Format example for docstring:\n",
    "\n",
    "            r\\\"\"\" \n",
    "                    Foo Method Description \n",
    "\n",
    "                :latex: MULTI-LINE\n",
    "                        LATEX EXPRESSION\n",
    "                        INSERTED HERE\n",
    "\n",
    "                :param bar: (example parameter)\n",
    "                :returns: (whatever) \n",
    "\n",
    "                * [Notes] : \n",
    "\n",
    "                    - latex code must be followed by \n",
    "                        at least one ':'(like the one \n",
    "                        leading param above)\n",
    "\n",
    "                    - doc string must be proceeded by r\\\"\"\" \n",
    "            \\\"\"\"\n",
    "\n",
    "    :param: method with string literal doc string containing latex\n",
    "    :returns: formated latex expression\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    latex_string = extract_doc_latex(method)\n",
    "    return display(Math(latex_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \n",
       "       \n",
       "               \\text{Takes as input paramater a matrix $\\mathbf{A}_{s \\times n}$,} \\\\\n",
       "               \\text{ }\\\\\n",
       "               \\mathbf{A}_{ input } =\n",
       "                \\begin{bmatrix}\n",
       "                     \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
       "                                & \\vdots     &            \\\\\n",
       "                     \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
       "                \\end{bmatrix}\n",
       "                \\text{ }\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\text{Where '$s$' is the number of instances being fed to the network.}\\\\\n",
       "                \\\\\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\text{Adds a bias column to the first column of $\\mathbf{A}$ and returns the new matrix $\\mathbf{A'}_{s \\times n+1}$, }\\\\\n",
       "                \\text{ }\\\\\n",
       "                \\mathbf{A'}_{column \\, bias} =\n",
       "                \\begin{bmatrix}\n",
       "                     1     &  \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
       "                   \\       &             & \\vdots     &            \\\\\n",
       "                     1     &  \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
       "                \\end{bmatrix}\n",
       "                \n",
       "       $$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_bias_unit(A):\n",
    "    r\"\"\"\n",
    "        Add  bias units (column of 1s) to array at index 0\n",
    "        \n",
    "       :latex: \n",
    "       \n",
    "               \\text{Takes as input paramater a matrix $\\mathbf{A}_{s \\times n}$,} \\\\\n",
    "               \\text{ }\\\\\n",
    "               \\mathbf{A}_{ input } =\n",
    "                \\begin{bmatrix}\n",
    "                     \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
    "                                & \\vdots     &            \\\\\n",
    "                     \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
    "                \\end{bmatrix}\n",
    "                \\text{ }\\\\\n",
    "                \\text{ }\\\\\n",
    "                \\text{Where '$s$' is the number of instances being fed to the network.}\\\\\n",
    "                \\\\\\\\\n",
    "                \\text{ }\\\\\n",
    "                \\text{Adds a bias column to the first column of $\\mathbf{A}$ and returns the new matrix $\\mathbf{A'}_{s \\times n+1}$, }\\\\\n",
    "                \\text{ }\\\\\n",
    "                \\mathbf{A'}_{column \\, bias} =\n",
    "                \\begin{bmatrix}\n",
    "                     1     &  \\text{---} & a^{(0)}    & \\text{---} \\\\\n",
    "                   \\       &             & \\vdots     &            \\\\\n",
    "                     1     &  \\text{---} & a^{(s-1)}  & \\text{---} \\\\\n",
    "                \\end{bmatrix}\n",
    "                \n",
    "       :param A: numpy array to add bias row or column to\n",
    "    \"\"\"\n",
    "    ones = np.ones((A.shape[0], 1))\n",
    "    A_new = np.hstack((ones, A))\n",
    "\n",
    "    return A_new\n",
    "\n",
    "latex_doc(add_bias_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\text{ Constructs Weight Matrix $\\mathbf{W}_{n \\times m}^{(l)}$. } \\\\ \\\\\n",
       "               \\text{ }\\\\\n",
       "               \\text{ Initalizes weights using a random uniform distribution ranging from $[-1,1]$.} \\\\\n",
       "               \\text{ Number of input units, } n  \\\\\n",
       "               \\text{ Number of output units, } m \\\\\n",
       "               \\text{ }\\\\\n",
       "               \\mathbf{W}^{(l)} =\n",
       "               \\begin{bmatrix}\n",
       "                    b_{0}   & \\ldots  & b_{m-1} \\\\\n",
       "                    w_{1,0} & \\ldots  & w_{1,m-1} \\\\\n",
       "                    \\vdots  & \\ldots  & \\vdots \\\\\n",
       "                    w_{n-1,0} & \\ldots  & w_{n-1,m-1}\n",
       "               \\end{bmatrix} \\\\\n",
       "    \\text{ This matrix is used in conjustion with layer $l's$ activation matrix $\\mathbf{A}^{(l)}$ }\\\\\n",
       "    \\text{ to calculate the input matrix $\\mathbf{Z}^{(l)}$ to the next activation function.} \\\\\n",
       "    \\\\\n",
       "    \\text{ }\\\\\n",
       "    \\text{ }\\\\\n",
       "    \\text{If '$s$' be the number of instances being fed to the network then,}\\\\\n",
       "    \\mathbf{Z}_{s \\times m}^{(l)} = \\mathbf{A}_{s \\times n}^{(l)} \\mathbf{W}_{n \\times m}^{(l)} \\\\\n",
       "    \\text{ }\\\\\n",
       "    \\mathbf{Z} =\n",
       "    \\begin{bmatrix} \n",
       "            \\text{---} &  z^{(0)}   & \\text{---} \\\\\n",
       "                       & \\ldots     &            \\\\\n",
       "            \\text{---} &  z^{(s-1)} & \\text{---} \\\\\n",
       "    \\end{bmatrix} , \\;\n",
       "    \n",
       "    \\begin{bmatrix} \n",
       "            1       & \\text{---} &  a^{(0)}   & \\text{---} \\\\\n",
       "           \\vdots   &            & \\ldots     &            \\\\\n",
       "            1       & \\text{---} &  a^{(s-1)} & \\text{---} \\\\\n",
       "    \\end{bmatrix}\n",
       "    = \\mathbf{A} \\\\\n",
       "    \n",
       "    \\text{ }\\\\\n",
       "    \\text{Returns randomly initalized weight matrix $\\mathbf{W}_{n \\times m}^{(l)}.$}\n",
       "    \n",
       "    \n",
       "      $$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_weight_matrix(num_input_units, num_output_units):\n",
    "    r\"\"\"\n",
    "        Build Weight Matrix W for layer l given number of inputs and outputs.\n",
    "        Initialize weights with small random numbers.\n",
    "          \n",
    "       :latex: \\text{ Constructs Weight Matrix $\\mathbf{W}_{n \\times m}^{(l)}$. } \\\\ \\\\\n",
    "               \\text{ }\\\\\n",
    "               \\text{ Initalizes weights using a random uniform distribution ranging from $[-1,1]$.} \\\\\n",
    "               \\text{ Number of input units, } n  \\\\\n",
    "               \\text{ Number of output units, } m \\\\\n",
    "               \\text{ }\\\\\n",
    "               \\mathbf{W}^{(l)} =\n",
    "               \\begin{bmatrix}\n",
    "                    b_{0}   & \\ldots  & b_{m-1} \\\\\n",
    "                    w_{1,0} & \\ldots  & w_{1,m-1} \\\\\n",
    "                    \\vdots  & \\ldots  & \\vdots \\\\\n",
    "                    w_{n-1,0} & \\ldots  & w_{n-1,m-1}\n",
    "               \\end{bmatrix} \\\\\n",
    "    \\text{ This matrix is used in conjustion with layer $l's$ activation matrix $\\mathbf{A}^{(l)}$ }\\\\\n",
    "    \\text{ to calculate the input matrix $\\mathbf{Z}^{(l)}$ to the next activation function.} \\\\\n",
    "    \\\\\n",
    "    \\text{ }\\\\\n",
    "    \\text{ }\\\\\n",
    "    \\text{If '$s$' be the number of instances being fed to the network then,}\\\\\n",
    "    \\mathbf{Z}_{s \\times m}^{(l)} = \\mathbf{A}_{s \\times n}^{(l)} \\mathbf{W}_{n \\times m}^{(l)} \\\\\n",
    "    \\text{ }\\\\\n",
    "    \\mathbf{Z} =\n",
    "    \\begin{bmatrix} \n",
    "            \\text{---} &  z^{(0)}   & \\text{---} \\\\\n",
    "                       & \\ldots     &            \\\\\n",
    "            \\text{---} &  z^{(s-1)} & \\text{---} \\\\\n",
    "    \\end{bmatrix} , \\;\n",
    "    \n",
    "    \\begin{bmatrix} \n",
    "            1       & \\text{---} &  a^{(0)}   & \\text{---} \\\\\n",
    "           \\vdots   &            & \\ldots     &            \\\\\n",
    "            1       & \\text{---} &  a^{(s-1)} & \\text{---} \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\mathbf{A} \\\\\n",
    "    \n",
    "    \\text{ }\\\\\n",
    "    \\text{Returns randomly initalized weight matrix $\\mathbf{W}_{n \\times m}^{(l)}.$}\n",
    "    \n",
    "    \n",
    "      :param num_input_units: integer, number of rows in W representing the number of outputs from the previous layer (neglecting the bias terms), or the number of features for the network inputs (again neglecting the bias terms).\n",
    "      :param num_output_units: integer, number of columns in W representing the number input units for the next layer (neglecting the bias terms), or the number of classes for the networks output.\n",
    "    \"\"\"\n",
    "    W_num_elems = num_input_units * num_output_units\n",
    "    \n",
    "    W = np.random.uniform(-1.0, 1.0, size=W_num_elems)\n",
    "    W = W.reshape(num_input_units, num_output_units) # reshape to be W\n",
    "    \n",
    "    return W\n",
    "\n",
    "latex_doc(build_weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class MLPBase(object):\n",
    "    \n",
    "    def __init__(self, n_layers, n_hidden_units=30, C=0.0, epochs=500, eta=0.001, phi=\"sigmoid\", objective_function=\"quadratic\", random_state=None):\n",
    "        \n",
    "        # [TODO]: reproduce results by initilizing W with the same weights\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # number of  layers for the network\n",
    "        self.n_layers_ = n_layers\n",
    "        \n",
    "        # number of units for each hidden layer\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        \n",
    "        # lambda value for l2 regularization\n",
    "        self.l2_C = C\n",
    "        \n",
    "        # maximum training epochs\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # activation function\n",
    "        self.phi = phi\n",
    "        \n",
    "        # objection function\n",
    "        self.objective_function = objective_function\n",
    "\n",
    "    def _initilize_weights(self):\n",
    "        \"\"\"\n",
    "            Build and initilize weight matrices for network.\n",
    "                \n",
    "            #\n",
    "            # dimensions of network components\n",
    "            #\n",
    "            \n",
    "            *let L be the number of layers\n",
    "            *let L-1 = out\n",
    "            *let s be the number of instances being fed through the network at once\n",
    "             (s should be the only variable parameter of a trained network)\n",
    "            \n",
    "            # W's - Total number of weight matrices = L: 0,1,...,L-1=out\n",
    "            W_0   is ( *n_features+1* x n_hidden_units-1 )\n",
    "            W_i   is ( n_hidden_units x n_hidden_units -1 ) for i = 1,..., L-2\n",
    "            W_out is ( n_hidden_units x *n_classes* )\n",
    "            \n",
    "            # A's - total number of A's (shit propigating through network) = L: 0,1,...,L-1=out\n",
    "            A_0   is ( s x n_features )\n",
    "            A_i   is ( s x n_hidden_units-1 ) for i = 1, ..., L-1=out\n",
    "            \n",
    "            # A_b's - bias column added at index 0 - total number of A_b's = L: 0,...,L-1=out\n",
    "            A_b_0 is ( s x n_features + 1 )\n",
    "            A_b_i is ( s x n_hidden_units ) for i = 1, ..., L-1=out\n",
    "            \n",
    "            # Z's - Z = A_prime @ W - total number of Z's = L: 0,...,L-1=out\n",
    "            Z_i is   ( s x n_hidden_units-1 ) for i = 0,...,L-2\n",
    "            Z_out is ( s x *n_classes_* )\n",
    "            \n",
    "            # Y_hat_prime - phi(Z_out) - total number = 1.\n",
    "            Y_hat_prime is ( s x n_classes_ )\n",
    "            \n",
    "            # Y_hat - each row becomes the index of the column with the max value - total number = 1.\n",
    "            Y_hat is ( s x 1 )\n",
    "            \n",
    "        \"\"\"\n",
    "        # tmp***\n",
    "        assert self.n_layers_ > 2\n",
    "        \n",
    "        #\n",
    "        # Total number of weight matrices is equal to the number of layers in the network\n",
    "        #  W's - Total number of weight matrices = L: 0,1,...,L-1=out\n",
    "        \n",
    "        #  W_0   is ( *n_features+1* x n_hidden_units-1 )\n",
    "        W_0 = build_weight_matrix( self.n_features_+1 , self.n_hidden_units-1 )\n",
    "        \n",
    "        #  W_i   is ( n_hidden_units x n_hidden_units -1 ) for i = 1,..., L-2\n",
    "        interior_Ws = [build_weight_matrix( self.n_hidden_units , self.n_hidden_units-1 ) for i in range(0,self.n_layers_-2)]\n",
    "        \n",
    "        #  W_out is ( n_hidden_units x *n_classes* )\n",
    "        W_out = build_weight_matrix( self.n_hidden_units , self.n_classes_ )\n",
    "        \n",
    "        # store initilized weight matrices\n",
    "        self.weight_matrices_ = [W_0] + interior_Ws + [W_out]\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y) #.values.T\n",
    "        return onehot\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear(z):\n",
    "        return z\n",
    "    \n",
    "    def _phi(self, z):\n",
    "        if self.phi == \"sigmoid\":\n",
    "            return self._sigmoid(z)\n",
    "        if self.phi == \"linear\":\n",
    "            return self._linear(z)\n",
    "        \n",
    "    def _phi_grad(self, A):\n",
    "        if self.phi == \"sigmoid\":\n",
    "            sigmoid_derivative = A * (1. - A)\n",
    "            return sigmoid_derivative\n",
    "        \n",
    "        if self.phi == \"linear\":\n",
    "            return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _objective_grad(self, y, y_hat):\n",
    "        obj_grad = -2*(y - y_hat) * self._phi_grad(y_hat)\n",
    "        return obj_grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def _MSE(Y_enc, Y_hat):\n",
    "        cost = np.mean((Y_enc-Y_hat)**2)\n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def _log_likelihood(Y_enc, Y_hat):\n",
    "        cost = np.sum(Y_enc * (np.log(Y_hat)))\n",
    "        return cost\n",
    "        \n",
    "    def _cost(self,Y_hat,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        if self.objective_function == \"quadratic\":\n",
    "            cost = self._MSE(Y_enc, Y_hat)\n",
    "        if self.objective_function == \"cross_entropy\":\n",
    "            cost = self._log_likelihood(Y_enc, Y_hat)\n",
    "        #cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(MLPBase):\n",
    "    \n",
    "    def _feedforward(self, X):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        \n",
    "        # store A's Ab's and Z's\n",
    "        As = []\n",
    "        Abs = []\n",
    "        Zs = []\n",
    "        \n",
    "        # set A0 = X and begin feed forward process\n",
    "        A0 = X\n",
    "        As += [A0]\n",
    "        \n",
    "        for W in self.weight_matrices_:\n",
    "            \n",
    "            # add a bias column to the last A\n",
    "            Ab = add_bias_unit( As[-1] )\n",
    "            \n",
    "            # store\n",
    "            Abs += [Ab]\n",
    "            \n",
    "            # calculate Z\n",
    "            Z = Ab @ W\n",
    "            \n",
    "            # store\n",
    "            Zs += [Z]\n",
    "            \n",
    "            # compute next A by\n",
    "            # applying activation function\n",
    "            A_next = self._phi(Z)\n",
    "            \n",
    "            # store\n",
    "            As += [A_next]\n",
    "            \n",
    "        # As should have one extra element then the other two lists\n",
    "        # this is the output of our network before one hot encoding\n",
    "        # Y_hat_prime = As[-1]\n",
    "        \n",
    "        return As, Abs, Zs\n",
    "            \n",
    "            \n",
    "    def _get_gradient(self, As, Abs, Zs, Y_enc):\n",
    "        #Compute gradient step using backpropagation.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # backpropagation\n",
    "        grad1 = np.zeros(W1.shape)\n",
    "        grad2 = np.zeros(W2.shape)\n",
    "        \n",
    "        # for each instance's activations \n",
    "        for (a1,a2,a3,y) in zip(A1.T,A2.T,A3.T,Y_enc.T):\n",
    "            \n",
    "            dJ_dz2 = self._objective_grad(y, a3)\n",
    "            dJ_dz1 = dJ_dz2 @ W2 @ np.diag(self._phi_grad(a2))\n",
    "                         \n",
    "            dz2_dw2 = a2[np.newaxis,:]\n",
    "            dz1_dw1 = a1[np.newaxis,:]\n",
    "            \n",
    "            # grad = VA.T \n",
    "            grad2 += dJ_dz2[:,np.newaxis]  @ dz2_dw2\n",
    "            grad1 += dJ_dz1[1:,np.newaxis] @ dz1_dw1\n",
    "            # don't incorporate bias term in the z1 gradient \n",
    "            # (its added in a2 from another layer)\n",
    "            \n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += (W1[:, 1:] * self.l2_C)\n",
    "        grad2[:, 1:] += (W2[:, 1:] * self.l2_C)\n",
    "\n",
    "        return grad1, grad2\n",
    "        \"\"\"\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Predict class labels\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        # copy data\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        \n",
    "        # setup one hot encoding\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_classes_ = Y_enc.shape[1]\n",
    "        self._initilize_weights()\n",
    "        \n",
    "        As, Abs, Zs = self._feedforward(X_data)\n",
    "        \n",
    "        i = 0\n",
    "        for A, Ab, W, Z in zip(As, Abs, self.weight_matrices_, Zs):\n",
    "            print(\"A_%s shape: %s\" % (i, A.shape))\n",
    "            print(\"Ab_%s shape: %s\" % (i, Ab.shape))\n",
    "            print(\"W_%s shape: %s\" % (i, W.shape))\n",
    "            print(\"Z_%s shape: %s\" % (i, Z.shape))\n",
    "            i+=1\n",
    "            \n",
    "        #\n",
    "        # Backprop\n",
    "        #\n",
    "        \n",
    "        # calculate the error starting at the last layer and moving backward\n",
    "        \n",
    "        def dphi_zi(Abs, i):\n",
    "            Aim1 = Abs[i-1]\n",
    "            return Aim1*(1. - Aim1)\n",
    "               \n",
    "        error = As[-1] - Y_enc\n",
    "        Vs = []\n",
    "        Vn = error * As[-1] * (1. - As[-1])\n",
    "        Vs += [Vn]\n",
    "        \n",
    "        def show_sizes(Vs, As, Ws):\n",
    "            i = self.n_layers_ - len(Vs)\n",
    "            Ai = Abs[i]\n",
    "            Wi = Ws[i]\n",
    "            Vip1 = Vs[-1]\n",
    "            print(\"\\n\")\n",
    "            print(\"Ab_%s shape: %s\" % (i, Ai.shape))\n",
    "            print(\"W_%s shape: %s\" % (i, Wi.shape))\n",
    "            print(\"V_%s shape: %s\" % (i+1, Vip1.shape))\n",
    "            print(\"dphi_z_%s shape: %s\" % (i, dphi_zi(Abs, i).shape))\n",
    "            \n",
    "            Vi = dphi_zi(Abs, i)*(Vip1 @ Wi.T)\n",
    "            print(\"V_%s shape: %s\" % (i, Vi.shape))\n",
    "            \n",
    "            Aim1 = Abs[i-1]\n",
    "            print(\"A_%s shape: %s\" % (i-1, Aim1.shape))\n",
    "            \n",
    "            grad_ip1 = Vip1.T @ Ai\n",
    "            print(grad_ip1.shape)\n",
    "            \n",
    "            grad_i = Vi.T @ Aim1\n",
    "            print(grad_i.shape)\n",
    "            \n",
    "            \n",
    "        Ws = self.weight_matrices_\n",
    "        show_sizes(Vs, As, Ws)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # As has one more value then the other list, its last element is A_out\n",
    "        # The activation of the output layer\n",
    "        A_out = As[-1]\n",
    "        dJ_dz1 = self._objective_grad(Y_enc, A_out)\n",
    "        \n",
    "        g_out = Abs[-1].T @ dJ_dz1\n",
    "        print(self.weight_matrices_[-1].shape)\n",
    "        print(g_out.shape)\n",
    "        \n",
    "        \n",
    "        W_m1 = self.weight_matrices_[-1]\n",
    "        A_m1 = Abs[-2]\n",
    "        \n",
    "        #dJ_dz2 = np.diag(A_m1*(1-A_m1)) * (dJ_dz1 @ W_m1.T) \n",
    "        #print(Abs[-2].T.shape)\n",
    "        #print(dJ_dz2.shape)\n",
    "        \n",
    "        #A_m2 = As[-3]\n",
    "        #g_m1 = A_m2.T @ dJ_dz2\n",
    "        print(self.weight_matrices_[-2].shape)\n",
    "        #print(g_m1.shape)\n",
    "        print((A_m1*(1-A_m1)).shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # W_out is the last weight matrix\n",
    "        W_m1 = self.weight_matrices_[-1]\n",
    "        A_m1 = Abs[-2]\n",
    "        \n",
    "        #dJ_dz1 = np.diag(A_m1*(1-A_m1)) * ( W_out.T @ dJ_dz2 )\n",
    "        dJ_dz2 = np.diag(A_m1*(1-A_m1)) * (dJ_dz1 @ W_m1.T)\n",
    "        print(dJ_dz2.shape)\n",
    "        \n",
    "        W_m2 = self.weight_matrices_[-2]\n",
    "        #A_m2 = Abs[-2]\n",
    "        A_m2 = As[-2]\n",
    "        dJ_dz3 = A_m2*(1-A_m2) * (dJ_dz2 @ W_m2)\n",
    "        \n",
    "        print(dJ_dz3.shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        #W_m3 = self.weight_matrices_[-3]\n",
    "        #A_m3 = As[-3]\n",
    "        #dJ_dz4 = A_m3*(1-A_m3) * (dJ_dz3 @ W_m3)\n",
    "        \n",
    "        #print(dJ_dz2.shape)\n",
    "        #print(W_m2.T.shape)\n",
    "        #t = dJ_dz2 @ W_m2\n",
    "        #print(t.shape)\n",
    "        #print(np.diag(A_m2*(1-A_m2)).shape)\n",
    "        #t2 = A_m2*(1-A_m2)\n",
    "        #print(t2.shape)\n",
    "        #dJ_dz3 = np.diag(A_m2*(1-A_m2)) * (dJ_dz2 @ W_m2.T)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Y_enc is the vector of true class lables\n",
    "        # A_out = Y_hat i think\n",
    "        \n",
    "        # calculate the sensitivity of the final layer\n",
    "        # lets denote this as V_fm0, V_fm1, ..., V_fm(n_layers-1)\n",
    "        V_fm0 = self._objective_grad(Y_enc, A_out)\n",
    "        \n",
    "        # W_out is the last weight matrix\n",
    "        W_out = self.weight_matrices_[-1]\n",
    "        \n",
    "        # We also need the activation of the last hidden layer (Abs[-1])\n",
    "        A_fm1 = Abs[-1]\n",
    "        W_fm0 = W_out\n",
    "        V_fm1 = np.dot(V_fm0, W_fm0.T) * self._phi_grad(A_fm1)\n",
    "    \n",
    "        # checking\n",
    "        grad_w_out = A_fm1.T @ V_fm0\n",
    "        grad_b_out = np.sum(V_fm0, axis=0)\n",
    "        \n",
    "        #continuing until it breaks...\n",
    "        A_fm2 = Abs[-2]\n",
    "        W_fm1 = self.weight_matrices_[-2]\n",
    "        V_fm2 = np.dot(V_fm1, W_fm1.T) #* self._phi_grad(A_fm2)\n",
    "        #grad_w_fm1 = \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # As has one more value then the other list, its last element is A_out\n",
    "        # The activation of the output layer\n",
    "        A_out = As[-1]\n",
    "        \n",
    "        # W_out is the last weight matrix\n",
    "        W_out = self.weight_matrices_[-1]\n",
    "        \n",
    "        # first calculate \n",
    "        \n",
    "        # the error vector of the output layer\n",
    "        \n",
    "        # A_out is the output of the network\n",
    "        # Y_enc is the vector of true class lables\n",
    "        \n",
    "        # (n_samples x n_classlables)\n",
    "        error = A_out - Y_enc\n",
    "        \n",
    "        # We also need the activation of the last hidden layer (Abs[-1])\n",
    "        # for calculating the gradient of the objective function\n",
    "        \n",
    "        # vectorized backpropagation\n",
    "        #V2 = -2*(Y_enc-A3)*A3*(1-A3)  # last layer sensitivity\n",
    "        #V1 = A2*(1-A2)*(W2.T @ V2) # back prop the sensitivity \n",
    "        \n",
    "       \n",
    "        # As has one more value then the other list, its last element is A_out\n",
    "        A_out = As[-1]\n",
    "        \n",
    "        # W_out is last matrix\n",
    "        W_out = self.weight_matrices_[-1]\n",
    "        \n",
    "       \n",
    "        # first calculate the error vector of the output layer\n",
    "        # A_out is the output of the network\n",
    "        # Y_enc is the vector of true class lables\n",
    "        # [n_samples, n_classlables]\n",
    "        sigma_out = A_out - Y_enc\n",
    "        \n",
    "        # [n_samples, n_hidden]\n",
    "        A_h = Abs[-1]\n",
    "        sigmoid_derivative_h = A_h * (1. - A_h)\n",
    "        \n",
    "        # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
    "        # -> [n_samples, n_hidden]\n",
    "        sigma_h = (np.dot(sigma_out, W_out.T) * sigmoid_derivative_h)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "\n",
    "        return self\n",
    "        \"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_layers=4,\n",
    "              n_hidden_units=30, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=200, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              phi=\"sigmoid\",\n",
    "              objective_function=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_0 shape: (1437, 64)\n",
      "Ab_0 shape: (1437, 65)\n",
      "W_0 shape: (65, 29)\n",
      "Z_0 shape: (1437, 29)\n",
      "A_1 shape: (1437, 29)\n",
      "Ab_1 shape: (1437, 30)\n",
      "W_1 shape: (30, 29)\n",
      "Z_1 shape: (1437, 29)\n",
      "A_2 shape: (1437, 29)\n",
      "Ab_2 shape: (1437, 30)\n",
      "W_2 shape: (30, 29)\n",
      "Z_2 shape: (1437, 29)\n",
      "A_3 shape: (1437, 29)\n",
      "Ab_3 shape: (1437, 30)\n",
      "W_3 shape: (30, 10)\n",
      "Z_3 shape: (1437, 10)\n",
      "\n",
      "\n",
      "Ab_3 shape: (1437, 30)\n",
      "W_3 shape: (30, 10)\n",
      "V_4 shape: (1437, 10)\n",
      "dphi_z_3 shape: (1437, 30)\n",
      "V_3 shape: (1437, 30)\n",
      "A_2 shape: (1437, 30)\n",
      "(10, 30)\n",
      "(30, 30)\n",
      "CPU times: user 26.7 ms, sys: 7.61 ms, total: 34.3 ms\n",
      "Wall time: 9.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = MLP(**params)\n",
    "nn.fit(X_train, y_train, print_progress=10)\n",
    "#yhat = nn.predict(X_test)\n",
    "#print('Test acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30, C=0.0, epochs=500, eta=0.001, phi=\"sigmoid\", objective_function=\"quadratic\", random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.phi = phi\n",
    "        self.objective_function = objective_function\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear(z):\n",
    "        return z\n",
    "    \n",
    "    def _phi(self, z):\n",
    "        if self.phi == \"sigmoid\":\n",
    "            return self._sigmoid(z)\n",
    "        if self.phi == \"linear\":\n",
    "            return self._linear(z)\n",
    "        \n",
    "    def _phi_grad(self, a):\n",
    "        if self.phi == \"sigmoid\":\n",
    "            return a*(1-a)\n",
    "        if self.phi == \"linear\":\n",
    "            return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _objective_grad(self, y, y_hat):\n",
    "        obj_grad = -2*(y - y_hat) * self._phi_grad(y_hat)\n",
    "        return obj_grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def _MSE(Y_enc, Y_hat):\n",
    "        cost = np.mean((Y_enc-Y_hat)**2)\n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def _log_likelihood(Y_enc, Y_hat):\n",
    "        #term1 = -Y_enc * (np.log(Y_hat))\n",
    "        #term2 = (1.0 - Y_enc) * np.log(1.0 - Y_hat)\n",
    "        #cost = np.sum(term1 - term2)\n",
    "        cost = np.sum(Y_enc * (np.log(Y_hat)))\n",
    "        return cost\n",
    "        \n",
    "    def _cost(self,Y_hat,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        if self.objective_function == \"quadratic\":\n",
    "            cost = self._MSE(Y_enc, Y_hat)\n",
    "        if self.objective_function == \"cross_entropy\":\n",
    "            cost = self._log_likelihood(Y_enc, Y_hat)\n",
    "        #cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
